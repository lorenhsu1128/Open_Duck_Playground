### 專案總覽

這個專案是一個使用 [MuJoCo](https://mujoco.org/) 物理模擬器和 [Brax](https://github.com/google/brax) 函式庫，對名為 "Open Duck Mini V2" 的雙足機器人進行強化學習（Reinforcement Learning）訓練的框架。其主要目標是訓練機器人學習執行不同的任務，例如：根據指令行走（`joystick` 任務）以及維持站立平衡（`standing` 任務）。

此專案包含了完整的機器人模型（XML 檔）、訓練環境設置、獎勵函數、訓練流程、以及將訓練好的模型匯出並進行推論（inference）的工具。

### PY 檔案功能詳解

以下是每個 `.py` 檔案的功能說明：

#### `playground/open_duck_mini_v2/` (機器人專屬程式碼)

  * **`base.py`**:

      * **功能**: 定義了 `OpenDuckMiniV2Env` 這個基礎環境類別，所有該機器人的特定任務（task）都會繼承這個類別。它處理 MuJoCo 模型的載入、讀取機器人關節和致動器（actuator）的資訊、以及提供讀取/設定機器人狀態（如位置、速度）的輔助函式。

  * **`constants.py`**:

      * **功能**: 存放專案中會用到的常數，例如 XML 模型的檔案路徑、機器人特定部位（如腳、軀幹）的名稱、以及感測器的名稱。這樣可以方便統一管理，避免在程式碼中寫死（hard-code）字串或數值。

  * **`custom_rewards.py`**:

      * **功能**: 實現了名為 `reward_imitation` 的客製化獎勵函數。這個函數的目的是讓機器人模仿一個預先定義好的參考動作（reference motion），透過比較機器人目前的動作與參考動作的差異來給予獎勵，藉此學習更自然的步態。

  * **`joystick.py`**:

      * **功能**: 定義了「搖桿（joystick）」任務的環境。在這個任務中，機器人需要學習根據一個隨機生成的命令（包含前進/後退、左/右平移、旋轉的速度）來移動。這個檔案也定義了這個任務專屬的觀察（observation）空間、獎勵函數（例如：追蹤目標速度、站立姿態、存活獎勵等）以及訓練結束的條件。

  * **`standing.py`**:

      * **功能**: 定義了「站立（standing）」任務的環境。這個任務的目標是訓練機器人在受到外部推力干擾的情況下，仍能保持平衡並站立不動。

  * **`mujoco_infer.py`**:

      * **功能**: 讓使用者可以在 MuJoCo 的互動式視窗中，載入一個訓練好的 ONNX 模型，並即時觀察機器人的推論結果。使用者可以透過鍵盤輸入來控制機器人的移動命令。

  * **`mujoco_infer_base.py`**:

      * **功能**: `mujoco_infer.py` 的基礎類別，負責處理 MuJoCo 模型的載入和機器人狀態的讀取，讓 `mujoco_infer.py` 可以專注在推論的邏輯上。

  * **`ref_motion_viewer.py`**:

      * **功能**: 一個用來視覺化參考動作的工具。它可以載入 `polynomial_coefficients.pkl` 檔案，並在 MuJoCo 視窗中顯示出機器人模仿該動作的樣子，方便開發者調試或觀察參考動作是否正確。

  * **`runner.py`**:

      * **功能**: `open_duck_mini_v2` 機器人的主要訓練腳本。它會根據命令列參數來選擇要執行的任務（`joystick` 或 `standing`），並啟動 `playground/common/runner.py` 中的訓練流程。

#### `playground/common/` (共用程式碼)

  * **`runner.py`**:

      * **功能**: 定義了 `BaseRunner` 類別，這是一個通用的訓練流程框架。它負責初始化環境、設定 PPO 訓練演算法的參數、啟動訓練、以及處理訓練過程中的回呼（callback）函式（例如：儲存模型、記錄訓練數據到 TensorBoard）。

  * **`export_onnx.py`**:

      * **功能**: 提供 `export_onnx` 函式，可以將 JAX/Flax 訓練出的模型權重，轉換並儲存成 ONNX (Open Neural Network Exchange) 格式。ONNX 是一個開放的模型格式，讓模型可以在不同的深度學習框架之間轉移。

  * **`onnx_infer.py`**:

      * **功能**: 提供 `OnnxInfer` 類別，方便載入 ONNX 模型並進行推論。`mujoco_infer.py` 就是使用這個類別來讀取模型。

  * **`plot_saved_obs.py`**:

      * **功能**: 一個用來繪製並分析儲存的觀察（observation）數據的腳本。當 `mujoco_infer.py` 執行時，會將每一步的觀察數據儲存成 `mujoco_saved_obs.pkl`，這個腳本可以讀取該檔案並用 `matplotlib` 繪製成圖表，方便分析機器人的狀態變化。

  * **`poly_reference_motion.py` / `poly_reference_motion_numpy.py`**:

      * **功能**: 這兩個檔案的功能相似，都是用來讀取並處理 `polynomial_coefficients.pkl` 參考動作檔。這個檔案儲存了用多項式擬合（polynomial fitting）的步態資料。`PolyReferenceMotion` 類別可以根據給定的移動指令（速度）和時間點，計算出對應的參考關節角度、速度等資訊。`_numpy` 版本是使用 NumPy，而另一個版本則是使用 JAX 的 `jax.numpy`。

  * **`randomize.py`**:

      * **功能**: 實現了「領域隨機化（Domain Randomization）」的功能。在訓練過程中，它會隨機改變模擬環境中的一些物理參數，例如：地板摩擦力、機器人各部位的質量、馬達的力臂等。這樣做可以讓訓練出來的模型更具備泛化能力（generalization），更能適應真實世界的物理變化。

  * **`rewards.py` / `rewards_numpy.py`**:

      * **功能**: 提供了一系列常用的獎勵/懲罰函數（reward/cost functions），例如：追蹤速度的獎勵、維持特定方向的懲罰、力矩（torque）消耗的懲罰、動作變化率的懲罰、維持存活的獎勵等等。`joystick.py` 和 `standing.py` 會根據任務需求來組合這些函數。

  * **`utils.py`**:

      * **功能**: 包含一些工具函式，例如 `LowPassActionFilter`。這是一個低通濾波器，可以用來平滑化神經網路輸出的動作指令，讓機器人的動作看起來更穩定、更不容易抖動。

### 命令列執行參數

以下是主要幾個可以執行的 `.py` 檔案以及它們的命令列參數：

#### 1\. `playground/open_duck_mini_v2/runner.py` (訓練)

  * **用途**: 啟動訓練流程。
  * **命令範例**:
    ```bash
    python playground/open_duck_mini_v2/runner.py --env joystick --task flat_terrain --output_dir my_checkpoints
    ```
  * **參數**:
      * `--output_dir`: 設定儲存模型權重（checkpoints）和 TensorBoard 紀錄檔的資料夾路徑。預設為 `"checkpoints"`。
      * `--num_timesteps`: 訓練的總時間步數（timesteps）。預設為 `150,000,000`。
      * `--env`: 選擇要訓練的環境。可選 `joystick` 或 `standing`。預設為 `"joystick"`。
      * `--task`: 選擇任務的場景。例如 `flat_terrain` (平坦地面) 或 `rough_terrain` (崎嶇地面)。預設為 `"flat_terrain"`。
      * `--restore_checkpoint_path`: 如果要從一個已經儲存的模型權重繼續訓練，可以指定該權重檔案的路徑。預設為 `None`。

#### 2\. `playground/open_duck_mini_v2/mujoco_infer.py` (推論)

  * **用途**: 使用 MuJoCo 互動視窗進行即時推論。
  * **命令範例**:
    ```bash
    python playground/open_duck_mini_v2/mujoco_infer.py -o path/to/your/model.onnx
    ```
  * **參數**:
      * `-o`, `--onnx_model_path`: **(必要)** 指定要載入的 ONNX 模型檔案路徑。
      * `--reference_data`: 參考動作資料 `polynomial_coefficients.pkl` 的路徑。預設為 `"playground/open_duck_mini_v2/data/polynomial_coefficients.pkl"`。
      * `--model_path`: MuJoCo 場景 XML 檔案的路徑。預設為 `"playground/open_duck_mini_v2/xmls/scene_flat_terrain.xml"`。
      * `--standing`: 如果設定了這個旗標，會進入站立模式。預設為 `False`。

#### 3\. `playground/open_duck_mini_v2/ref_motion_viewer.py` (參考動作觀察)

  * **用途**: 在 MuJoCo 中觀看參考動作。
  * **命令範例**:
    ```bash
    python playground/open_duck_mini_v2/ref_motion_viewer.py --command 0.1 0.0 0.0
    ```
  * **參數**:
      * `--reference_data`: 參考動作資料 `polynomial_coefficients.pkl` 的路徑。預設為 `"playground/go_bdx/data/polynomial_coefficients.pkl"`。
      * `-joystick`: 如果設定了這個旗標，並且電腦有連接搖桿，則可以使用搖桿來控制命令。
      * `--command`: 以三個浮點數設定參考動作的命令 `[dx, dy, dtheta]` (X軸速度, Y軸速度, 旋轉速度)。預設為 `[0.0, -0.05, -0.1]`。
      * `--scene`: 選擇要載入的場景。預設為 `"flat_terrain"`。

#### 4\. `playground/common/plot_saved_obs.py` (繪製觀察數據)

  * **用途**: 繪製 `mujoco_infer.py` 執行後儲存的 `mujoco_saved_obs.pkl`。
  * **命令範例**:
    ```bash
    python playground/common/plot_saved_obs.py -d mujoco_saved_obs.pkl
    ```
  * **參數**:
      * `-d`, `--data`: 指定要讀取的 `.pkl` 檔案路徑。預設為 `"mujoco_saved_obs.pkl"`。


### 透過修改命令列參數和程式碼中的特定檔案來增加訓練量與調整隨機訓練參數。

### 1\. 增加總訓練量

增加總訓練量最直接的方法是修改訓練腳本的命令列參數，或是直接調整 PPO 演算法的超參數。

#### 方法一：透過命令列參數 (建議)

在執行訓練時，您可以直接調整 `--num_timesteps` 參數來設定訓練的總步數。總步數越多，代表機器人與環境互動的次數越多，也就是訓練量越大。

  * **檔案**: `playground/open_duck_mini_v2/runner.py`
  * **參數**: `--num_timesteps`
  * **說明**: 這個參數定義了整個訓練過程將執行的環境互動總步數。預設值是 `150,000,000`。
  * **範例**: 將訓練總量增加到 3 億步。
    ```bash
    uv run playground/open_duck_mini_v2/runner.py --task flat_terrain --num_timesteps 300000000
    ```

#### 方法二：修改 PPO 超參數

如果您想更細緻地控制每一輪訓練的資料量，可以修改 PPO 演算法的參數，特別是 `num_envs`。

  * **檔案**: `playground/common/runner.py`
  * **參數**: `num_envs`
  * **說明**: 這個參數設定了要**同時平行運行的模擬環境數量**。越多的平行環境代表在同一個時間單位內可以收集到越多的訓練數據，進而增加訓練量，但也會消耗更多的 GPU 記憶體。預設值是 `8192`。
  * **修改位置**: 在 `playground/common/runner.py` 檔案中，找到 `locomotion_params.brax_ppo_config` 這一行。您可以覆寫其預設值，例如：
    ```python
    # in playground/common/runner.py

    def train(self) -> None:
        self.ppo_params = locomotion_params.brax_ppo_config(
            "BerkeleyHumanoidJoystickFlatTerrain"
        )
        self.ppo_training_params = dict(self.ppo_params)
        
        # 在這裡增加或修改參數
        self.ppo_training_params["num_envs"] = 16384  # 例如，將平行環境數量加倍
        
        # ... 後續程式碼 ...
    ```

-----

### 2\. 調整隨機訓練參數 (Domain Randomization)

「領域隨機化」是在訓練過程中隨機改變模擬器的物理參數，目的是為了讓訓練出的模型更具備泛化能力，更能適應真實世界的變化。所有的隨機化邏輯都定義在 `domain_randomize` 函式中。

  * **檔案**: `playground/common/randomize.py`
  * **函式**: `domain_randomize`

您可以直接修改此檔案中的 `minval` 和 `maxval` 來調整各項參數的隨機範圍。以下是可供調整的主要參數：

  * **地板摩擦力 (Floor Friction)**

      * **用途**: 模擬不同材質的地板。
      * **程式碼**:
        ```python
        geom_friction = model.geom_friction.at[FLOOR_GEOM_ID, 0].set(
            jax.random.uniform(key, minval=0.5, maxval=1.0)  # 原範圍是 0.4 到 1.0
        )
        ```

  * **關節靜摩擦力 (Static Friction)**

      * **用途**: 模擬關節的阻力變化。
      * **程式碼**:
        ```python
        frictionloss = model.dof_frictionloss[dof_addr] * jax.random.uniform(
            key, shape=(model.nu,), minval=0.9, maxval=1.1
        )
        ```

  * **馬達電樞 (Motor Armature)**

      * **用途**: 模擬馬達內部電阻或慣性的變化。
      * **程式碼**:
        ```python
        armature = model.dof_armature[dof_addr] * jax.random.uniform(
            key, shape=(model.nu,), minval=1.0, maxval=1.05
        )
        ```

  * **質心位置 (Center of Mass Position)**

      * **用途**: 模擬機器人重心因組裝或負重造成的偏移。
      * **程式碼**:
        ```python
        dpos = jax.random.uniform(key, (3,), minval=-0.05, maxval=0.05)
        body_ipos = model.body_ipos.at[TORSO_BODY_ID].set(
            model.body_ipos[TORSO_BODY_ID] + dpos
        )
        ```

  * **連桿質量 (Link Masses)**

      * **用途**: 模擬機器人各部位零件的製造公差。
      * **程式碼**:
        ```python
        dmass = jax.random.uniform(key, shape=(model.nbody,), minval=0.9, maxval=1.1)
        body_mass = model.body_mass.at[:].set(model.body_mass * dmass)
        ```

  * **軀幹額外質量 (Added Torso Mass)**

      * **用途**: 模擬機器人背負不同重量的物體。
      * **程式碼**:
        ```python
        dmass = jax.random.uniform(key, minval=-0.1, maxval=0.1)  # 原範圍是 -0.2 到 0.2
        body_mass = body_mass.at[TORSO_BODY_ID].set(body_mass[TORSO_BODY_ID] + dmass)
        ```

  * **初始關節角度 (Initial Joint Positions)**

      * **用途**: 讓機器人從一個稍微不同的初始站姿開始。
      * **程式碼**:
        ```python
        qpos0 = qpos0.at[joint_addr].set(
            qpos0[joint_addr]
            + jax.random.uniform(
                key, shape=(model.nu,), minval=-0.03, maxval=0.03
            )  # 原範圍是 -0.05 到 0.05
        )
        ```

  * **馬達增益 (Motor KP Gain)**

      * **用途**: 模擬馬達控制器的力度差異。
      * **程式碼**:
        ```python
        factor = jax.random.uniform(
            key, shape=(model.nu,), minval=0.9, maxval=1.1
        )  # 原範圍是 0.8 到 1.2
        ```

### Domain Randomization 每個參數的範圍上下限應該設定多少
這是一個非常好且關鍵的問題。Domain Randomization 參數範圍的設定直接影響到訓練的穩定性和最終模型（policy）的泛化能力（robustness），也就是所謂的「Sim-to-Real」——從模擬環境轉移到真實世界的效果。

設定這些範圍沒有一個絕對的「正確答案」，它更像是一門藝術，需要根據目標、機器人硬體特性以及不斷的實驗來調整。不過，我可以提供一套設定這些參數的**指導原則**和**建議範圍**。

### 設定範圍的核心原則

1.  **從現實出發 (Anchor in Reality)**：隨機化的範圍應該圍繞著您對真實世界機器人的最佳估計。例如，如果您知道機器人馬達的力矩誤差大約是 ±10%，那麼您的隨機化範圍就應該涵蓋這個區間。

2.  **循序漸進 (Curriculum Learning)**：一開始就使用非常大的隨機範圍，可能會讓學習任務變得過於困難，導致模型無法收斂。一個常見且有效的策略是：

      * **初期**：使用較小的範圍進行訓練，讓模型先學會基本的技能。
      * **中期**：在模型穩定後，逐漸增大隨機化的範圍，強迫模型去適應更大的變化。
      * **後期**：甚至可以根據模型在特定方面的弱點，針對性地加強某幾項參數的隨機化。

3.  **觀察訓練過程**：

      * **如果訓練很不穩定，獎勵值（reward）無法穩定上升**：這通常意味著隨機化的範圍太大了，任務太難。您應該縮小範圍。
      * **如果訓練非常順利，獎勵值很高，但在真實機器人上表現很差**：這就是典型的「Sim-to-Real Gap」，表示您的模擬環境太單一，隨機化範圍太小，沒有涵蓋真實世界的物理變化。您應該擴大範圍。

### 各項參數的建議範圍與說明

以下是針對 `playground/common/randomize.py` 檔案中各個參數的具體建議。我將其分為「**保守起始範圍**」和「**積極泛化範圍**」。建議您從保守範圍開始訓練。

-----

| 參數 | 用途 | 保守起始範圍 | 積極泛化範圍 | 說明 |
| :--- | :--- | :--- | :--- | :--- |
| **地板摩擦力**\<br\>`geom_friction` | 模擬不同材質的地面，如木地板、磁磚、地毯等。 | `[0.5, 1.0]` | `[0.4, 1.5]` | 摩擦係數很少會低於 0.4 或高於 1.5。範圍太大可能會產生不真實的物理行為。目前的 `[0.5, 1.0]` 是一個很好的起點。 |
| **關節靜摩擦力**\<br\>`frictionloss` | 模擬關節內部的阻力、潤滑程度的差異。 | `* [0.9, 1.1]`\<br\>(±10%) | `* [0.7, 1.3]`\<br\>(±30%) | 這個值代表與模型預設值的乘法因子。±10% 的範圍適合初期訓練。如果發現機器人在真實世界中關節比模擬中更「澀」或更「滑」，可以擴大此範圍。 |
| **馬達電樞**\<br\>`armature` | 模擬馬達內部電阻和慣性的變化，影響關節的響應速度和阻尼。 | `* [1.0, 1.05]`\<br\>(+0\~5%) | `* [0.95, 1.1]`\<br\>(±5\~10%) | 電樞值對穩定性影響較大，建議從小範圍開始，且通常是往上加比較符合真實情況（真實馬達有更多非理想效應）。 |
| **質心位置**\<br\>`body_ipos` | 模擬因組裝公差、額外負重（如感測器）導致的重心偏移。 | `± [0.01, 0.01, 0.01]`\<br\>(±1 cm) | `± [0.03, 0.03, 0.02]`\<br\>(X,Y ±3cm, Z ±2cm) | 對於小型雙足機器人，幾公分的重心偏移已經非常顯著。Z 軸（垂直）的偏移可以小一些，因為機器人對垂直重心的變化更敏感。 |
| **連桿質量**\<br\>`body_mass` | 模擬各部位零件的製造公差。 | `* [0.95, 1.05]`\<br\>(±5%) | `* [0.85, 1.15]`\<br\>(±15%) | ±5% 是一個比較貼近真實 3D 列印或零件製造公差的範圍。如果您的機器人結構很堅固，可以保持小範圍；如果結構較鬆散，可以嘗試更大的範圍。 |
| **軀幹額外質量**\<br\>`body_mass` (Torso) | 模擬機器人攜帶不同重量的負載（例如，更大的電池或額外的電腦）。 | `± 0.1` kg\<br\>(±100g) | `± 0.25` kg\<br\>(±250g) | 這個範圍完全取決於您預期機器人會負載多重的物體。目前的 `±0.1` kg 是個合理的開始。 |
| **初始關節角度**\<br\>`qpos0` | 讓機器人從稍微不同的初始姿態開始訓練，避免過擬合到單一站姿。 | `± 0.03` rad\<br\>(約 ±1.7°) | `± 0.08` rad\<br\>(約 ±4.5°) | 角度的擾動不需要太大，幾度的隨機偏移已經能有效增加訓練的泛化性。 |
| **馬達增益 (KP)**\<br\>`actuator_gainprm` | 模擬馬達 PID 控制器中 P (Proportional) 增益的差異。 | `* [0.9, 1.1]`\<br\>(±10%) | `* [0.8, 1.2]`\<br\>(±20%) | 這個參數直接影響馬達的「硬度」和響應。±10% 的範圍可以模擬不同馬達之間的性能差異。±20% 則是一個更具挑戰性的範圍。 |

### 總結與建議流程

1.  **開始**：使用上表中的「**保守起始範圍**」來設定 `playground/common/randomize.py` 檔案。
2.  **訓練**：執行訓練，並使用 TensorBoard 密切觀察 reward 曲線。如果曲線能夠穩定上升，說明模型正在學習。
3.  **評估**：當模型收斂後，將其部署到真實機器人上進行測試（如果有的話），或者在模擬中給予更困難的挑戰（例如，崎嶇地形 `rough_terrain`）。
4.  **迭代**：
      * 如果 Sim-to-Real 效果不佳，逐步向「**積極泛化範圍**」擴大參數，然後重新訓練。
      * 如果在擴大範圍後訓練變得不穩定，可以嘗試稍微縮小範圍，或者增加總訓練量 (`--num_timesteps`)，給模型更多的時間去學習和適應。

透過這樣不斷實驗和調整的過程，您就能找到最適合您機器人和任務的 Domain Randomization 參數設定。

### LOG內容詳細解讀
將會詳細解釋所有LOG內容，並教您如何從中判斷訓練結果的好壞。

### Log 內容逐段詳解

我將以您第一次訓練的 LOG 為例進行說明，因為兩次的結構是完全一樣的。

#### 1\. 啟動與環境設定 (開頭部分)

```
2025-09-09 22:26:04.050337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory...
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1757427964.102760      533 cuda_dnn.cc:8579] Unable to register cuDNN factory...
...
W0000 00:00:1757427964.222262      533 computation_placer.cc:177] computation placer already registered...
```

  * **內容意思**：這部分是 JAX 和 TensorFlow 在初始化時的日誌。這些 `Error (E)` 和 `Warning (W)` 訊息是**正常且無害的**。它們表示程式中的多個部分（JAX 和 TF）都嘗試去註冊同一個 CUDA 函式庫，但因為已經被註冊過了，所以後續的嘗試會跳出警告。這不影響程式的正常運行。

-----

#### 2\. MuJoCo 環境載入

```
xml: /home/loren/bdx/Open_Duck_Playground/playground/open_duck_mini_v2/xmls/scene_flat_terrain.xml
actuators: ['left_hip_yaw', 'left_hip_roll', ...]
joints: ['floating_base', 'left_hip_yaw', ...]
backlash joints: []
...
```

  * **內容意思**：這表示程式成功讀取了 MuJoCo 的模型檔案 (`.xml`)，並且正確解析出機器人的所有致動器（actuators）和關節（joints）資訊。這是訓練開始前的必要準備工作。

-----

#### 3\. PPO 演算法參數設定

```
Observation size: 101
PPO params: {'action_repeat': 1, 'batch_size': 256, ..., 'num_timesteps': 300000000, ...}
```

  * **內容意思**：這裡列出了本次訓練使用的 PPO 強化學習演算法的所有**超參數 (Hyperparameters)**。
      * `Observation size: 101`：代表神經網路模型每一次的輸入會有 101 個數值。
      * `PPO params`: 包含了學習率 (`learning_rate`)、同時運行的環境數量 (`num_envs: 8192`)、以及您從命令列輸入的總訓練步數 (`num_timesteps: 300000000`) 等重要設定。

-----

#### 4\. 訓練進度回報 (核心部分)

這是判斷訓練好壞最重要的部分，它會週期性地出現：

```
-----------
STEP: 21463040 reward: 219.75445556640625 reward_std: 154.60870361328125
-----------
```

  * **`STEP`**: 代表當前訓練的總步數（timesteps）。這個數字會一直增加，直到達到您設定的 `num_timesteps`。
  * **`reward` (獎勵值)**: **這是評估訓練成效最重要的指標**。它代表在目前的訓練階段，模型在一次完整的模擬（一個 episode）中，平均可以獲得多少分數。**分數越高，代表機器人的表現越好**。
  * **`reward_std` (獎勵標準差)**: 這個值代表了模型表現的**穩定性**。
      * **值越小**，代表每次模擬得到的分數都很接近平均值，模型表現很穩定。
      * **值越大**，代表模型表現時好時壞，不夠穩定。

-----

#### 5\. 模型儲存與匯出

```
Saving checkpoint (step: 21463040): /home/loren/bdx/.../2025_09_09_223111_21463040
 === EXPORT ONNX ===
...
Transferring Dense layer hidden_0, kernel shape (101, 512), bias shape (512,)
...
Weights transferred successfully.
Tensorflow prediction: [ 0.16337563 -0.08779149 ... ]
```

  * **內容意思**：
      * `Saving checkpoint`: 程式將當前的模型權重儲存到您的 `my_checkpoints` 資料夾中。檔名包含了日期和當前的訓練步數，方便您之後選用不同階段的模型。
      * `=== EXPORT ONNX ===`: 開始執行將模型轉換成 ONNX 格式的流程。
      * `Tensorflow prediction`: 這是用匯出後的 ONNX 模型進行一次測試推論的結果，主要是用來確認匯出過程沒有出錯，這個數值本身不直接反映模型的好壞。

### 如何判斷訓練結果的好與壞？

判斷的標準非常直接：**觀察 `reward` 的變化趨勢**。

#### 📈 一個好的訓練過程：

  * **`reward` 持續穩定上升**：隨著 `STEP` 的增加，`reward` 的平均值應該要有一個明顯的上升趨勢。這代表機器人確實從經驗中學到了如何獲得更高的分數。
  * **`reward_std` 相對較小或逐漸變小**：在訓練後期，標準差 `reward_std` 相對於 `reward` 的比例如果能縮小，代表模型學到的策略是穩定且可重複的。

#### 📉 一個壞的訓練過程：

  * **`reward` 停滯不前**：訓練了很久，但 `reward` 一直在一個很低的分數徘徊，沒有上升的跡象。
  * **`reward` 劇烈震盪或崩潰**：`reward` 上升到一定程度後，突然暴跌，並且再也無法回升。這可能是學習率過高或獎勵函數設計不良導致的。

-----

### 分析您兩次的訓練結果

#### 第一次訓練 (22:26 開始)

| STEP | reward (平均獎勵) |
| :--- | :--- |
| 0 | 13.9 |
| 21,463,040 | 219.7 |
| 42,926,080 | 220.4 |
| 64,389,120 | 242.6 |
| 85,852,160 | 267.9 |
| 107,315,200| 275.9 |
| ... | ... |
| **300,482,560**| **252.9** |

  * **分析**：這是一次**非常成功的訓練**！`reward` 從一開始的 13.9，一路飆升到 270 分以上。雖然在訓練末期略有下降，但整體趨勢非常健康，表明模型已經學會了相當不錯的行走策略。

#### 第二次訓練 (05:35 開始)

| STEP | reward (平均獎勵) |
| :--- | :--- |
| 0 | 14.1 |
| 21,463,040 | 219.6 |
| 42,926,080 | 209.0 |
| 64,389,120 | 239.0 |
| 85,852,160 | 272.2 |
| 107,315,200| 269.8 |
| ... | ... |
| **300,482,560**| **248.8** |

  * **分析**：這同樣是一次**成功的訓練**。由於強化學習的隨機性，每次訓練的曲線不盡相同是正常的。這次的 `reward` 也展現了強勁的上升趨勢，最高達到了 303 分，最終穩定在 248 分左右，與第一次的結果相當，證明了您的訓練設定是有效的。

### 總結與下一步

恭喜您！您的兩次訓練都取得了很好的成果。

  * **判斷依據**：主要看 `reward` 是否隨 `STEP` 穩定增長。
  * **視覺化工具**：雖然從文字 LOG 可以判斷，但更專業的方式是使用 **TensorBoard**。您可以在訓練時另外開啟一個終端機，執行 `uv run tensorboard --logdir=my_checkpoints`，就可以在網頁中看到平滑的 `reward` 曲線圖，一目了然。
  * **下一步**：現在您擁有了多個訓練好的模型（儲存在 `my_checkpoints` 資料夾中），可以挑選一個 `reward` 最高的 `.onnx` 檔案，使用 `mujoco_infer.py` 腳本來實際看看機器人學會了什麼樣的走路姿態！


### 要怎麼使用 mujoco_infer.py 腳本來實際看看機器人學會了什麼樣的走路姿態
使用 `mujoco_infer.py` 腳本來觀察您訓練好的模型非常簡單。這個腳本會打開一個 MuJoCo 的互動視窗，載入您的模型，然後您就可以像玩遊戲一樣用鍵盤即時控制機器人。

以下是詳細的步驟：

### 步驟一：挑選您最滿意的模型

首先，您需要從 `my_checkpoints` 資料夾中，挑選一個您認為訓練得最好的模型。判斷的標準就是看訓練 LOG 中哪個模型的 `reward` 分數最高。

根據您提供的 LOG，`STEP: 279019520` 時的 `reward` 達到了 `299.3`，這是一個非常高的分數。對應的 checkpoint 檔案應該是 `2025_09_09_231820_279019520.onnx`。我們就用這個模型來當作範例。

### 步驟二：執行推論 (Inference) 腳本

請打開您的終端機，確保您在 `Open_Duck_Playground` 這個專案的根目錄下，並且已經啟動了 `uv` 的虛擬環境 (如果沒有，請先執行 `source .venv/bin/activate`)。

然後，執行以下指令：

```bash
uv run playground/open_duck_mini_v2/mujoco_infer.py -o my_checkpoints/2025_09_09_231820_279019520.onnx
```

#### 指令解釋：

  * `uv run playground/open_duck_mini_v2/mujoco_infer.py`: 執行推論腳本。
  * `-o my_checkpoints/2025_09_09_231820_279019520.onnx`: 這是**最重要**的參數，`-o` (或 `--onnx_model_path`) 用來指定您要載入的 `.onnx` 模型檔案的路徑。

執行後，您應該會看到一個 MuJoCo 的模擬視窗彈出，裡面有您的 Open Duck Mini V2 機器人。

### 步驟三：在模擬中控制機器人

現在，您可以用鍵盤來給機器人下達移動指令，觀察它學會的走路姿態。

#### 主要控制模式（移動身體）：

| 按鍵 | 功能 |
| :--- | :--- |
| **↑** (上箭頭) | 前進 |
| **↓** (下箭頭) | 後退 |
| **←** (左箭頭) | 向左平移 |
| **→** (右箭頭) | 向右平移 |
| **Q** | 向左旋轉 |
| **E** | 向右旋轉 |
| **H** | **切換到頭部控制模式** |

#### 頭部控制模式：

當您按下 **H** 鍵後，會切換到頭部控制模式，此時的按鍵功能會改變：

| 按鍵 | 功能 |
| :--- | :--- |
| **↑** (上箭頭) | 抬頭（Pitch） |
| **↓** (下箭頭) | 低頭（Pitch） |
| **←** (左箭頭) | 向左轉頭（Yaw） |
| **→** (右箭頭) | 向右轉頭（Yaw） |
| **Q** | 向左歪頭（Roll） |
| **E** | 向右歪頭（Roll） |
| **H** | **切換回主要控制模式** |

### 總結

您只需要簡單的三個步驟：**挑選模型 -\> 執行指令 -\> 鍵盤控制**，就可以親眼看到您花費數小時訓練出來的成果了。試著操作機器人前後左右移動和轉彎，觀察它的步態是否流暢、平衡感如何，這是在強化學習中最有成就感的一刻！


### 要怎麼判讀附件的tensor board圖表
詳細解讀這張圖表的內容，以及如何從中判斷訓練成效。

### 圖表基本元素解析

* **橫軸 (X軸) - Step**:
    * 代表訓練的總步數 (timesteps)，也就是機器人與環境互動的總次數。
    * 這個軸從左到右代表時間的推移和訓練的進展。您可以看到最大值約為 300M，也就是 3 億步，對應您設定的 `--num_timesteps 300000000`。

* **縱軸 (Y軸) - Value**:
    * 代表您所觀察的指標數值。在這張圖中，它代表的是「獎勵值 (Reward)」。

* **曲線 (eval/episode\_reward)**:
    * 這是最重要的曲線，全名是「評估階段的平均單集獎勵 (Evaluation Episode Reward)」。
    * 它顯示了隨著訓練的進行，您的機器人（智慧體）在完成一次完整任務（一個 episode）時，平均能獲得的總分數。
    * **這條曲線是判斷模型好壞最核心的依據。**

* **淡色區域 (平滑後)**:
    * 圖中顏色較淺、半透明的區域代表原始數據的波動範圍或標準差。
    * 顏色較深、實心的線條是經過「平滑 (Smoothing)」處理後的趨勢線。我們在判讀時主要關注這條平滑後的趨勢線。

### 如何判讀這張圖表的訓練結果

從這張圖表來看，這是一次**非常成功的訓練**，原因如下：

1.  **明顯的上升趨勢**：
    * 在訓練初期（大約 0 到 50M 步之間），`eval/episode_reward` 曲線快速且陡峭地向上攀升。這表示機器人處於一個「快速學習期」，它很快就從隨機亂動的狀態，學會了如何站立、移動以獲得基礎分數。

2.  **進入高原期與持續優化**：
    * 在大約 50M 步之後，獎勵值的上升速度開始減緩，進入一個「高原期 (plateau)」。這在強化學習中非常正常，代表模型已經掌握了主要技能。
    * 即便如此，從 50M 步到約 200M 步，獎勵曲線仍然在緩慢但穩定地向上爬升。這表示模型正在進行「微調和優化」，學習更精細、更有效率的動作來獲得更高的分數。

3.  **收斂與穩定**：
    * 在訓練的後期（約 200M 步之後），獎勵曲線趨於平穩，大約在 250-300 分之間波動。這代表模型的性能已經達到了**收斂 (convergence)** 狀態。
    * 在當前的環境設定和獎勵函數下，模型的能力已經達到了極限，再增加訓練時間可能也無法顯著提升分數。這通常意味著訓練可以結束了。

### 總結

* **訓練是好是壞？**
    * **好的**。因為獎勵曲線展現了從快速學習到穩定收斂的完整過程，最終達到了一個相當高的分數。

* **哪個模型最好？**
    * 不一定是分數最高的那個點。通常我們會選擇**進入收斂區域後，表現最穩定的那個模型**。從圖上看，大約在 **200M 到 300M 步之間**產生的模型（checkpoints）都是非常好的候選者。您可以從這個區間挑選幾個模型，實際放到 `mujoco_infer.py` 中觀察它們的走路姿態，來決定哪一個最符合您的期望。

總之，TensorBoard 圖表是一個教科書等級的訓練範例。它清楚地展示了強化學習「從無到有」的學習過程。


### episode_cost 代表甚麼意思?
`episode_cost` (單集成本) 的概念與 `episode_reward` (單集獎勵) 剛好相反，它們共同構成了評估模型好壞的完整畫面。

### `episode_cost` 的核心意義

在強化學習中，我們通常設定一個「獎勵函數」來引導智慧體（Agent，也就是您的機器人模型）學習。這個函數的目標是：

  * **獎勵 (Reward)**：對好的行為給予**正**分數。
  * **懲罰/成本 (Cost/Penalty)**：對不好的行為給予**負**分數。

智慧體的目標是學習一套策略（Policy），使其在一個任務（episode）中獲得的**總分最高**。

**`episode_cost`** 就是將所有**負向的、懲罰性質的分數**加總起來的結果。它代表了機器人在完成一次任務的過程中，付出了多少「代價」。

簡單來說：

  * **`episode_reward`**：您希望看到的**好事**，數值越高越好。
  * **`episode_cost`**：您不希望看到的**壞事**，數值越低越好。

### `episode_cost` 在您的專案中是如何定義的？

在您的程式碼中，這個獎勵與成本的區分是在 `playground/open_duck_mini_v2/joystick.py` 檔案中定義的。

1.  **獎勵與成本的權重設定**：
    在 `joystick.py` 的 `default_config` 函式中，有一個 `reward_config` 區塊：

    ```python
    reward_config=config_dict.create(
        scales=config_dict.create(
            tracking_lin_vel=2.5,      # 正數 -> 獎勵
            tracking_ang_vel=6.0,      # 正數 -> 獎勵
            torques=-1.0e-3,           # 負數 -> 成本
            action_rate=-0.5,          # 負數 -> 成本
            stand_still=-0.2,          # 負數 -> 成本
            alive=20.0,                # 正數 -> 獎勵
            imitation=1.0,             # 正數 -> 獎勵
        ),
        # ...
    ),
    ```

    您可以看到，有些項目的權重（scales）是正數，有些是負數。

2.  **TensorBoard 的日誌邏輯**：
    在 `joystick.py` 的 `step` 函式中，有一段程式碼會根據權重的正負，將指標分別記錄到 TensorBoard 的不同類別下：

    ```python
    for k, v in rewards.items():
        rew_scale = self._config.reward_config.scales[k]
        if rew_scale != 0:
            if rew_scale > 0: # 如果權重是正數
                state.metrics[f"reward/{k}"] = v
            else: # 如果權重是負數
                state.metrics[f"cost/{k}"] = -v
    ```

      * 如果權重是**正數**，它會被歸類為 `reward`。
      * 如果權重是**負數**，它會被歸類為 `cost`。

因此，在您的 `joystick` 任務中，`episode_cost` 主要由以下幾個部分組成：

  * **`cost/torques`**: 懲罰機器人使用過大的馬達力矩（Torque）。這可以鼓勵機器人學習更節能、更柔順的動作。
  * **`cost/action_rate`**: 懲罰機器人做出劇烈、不連續的動作。這可以讓機器人的動作看起來更平滑、穩定。
  * **`cost/stand_still`**: 當指令是站立不動時，懲罰任何多餘的移動。這可以讓機器人在需要時保持穩定。

### 如何判讀 `episode_cost` 圖表？

判讀 `episode_cost` 的方式與 `episode_reward` **完全相反**。

  * **一個好的訓練過程**，您會希望看到 `episode_cost` 的曲線**持續穩定地下降**。這代表您的機器人不僅學會了如何完成任務（獲得 reward），還學會了如何**更有效率、更穩定、更節能**地完成任務。
  * **如果 `episode_cost` 居高不下或持續上升**，即使 `reward` 很高，也可能代表機器人學會了一種「暴力」的、高耗能、不穩定的方式來達成目標。

將 `episode_reward` 和 `episode_cost` 的圖表放在一起看，就能最全面地評估模型的整體表現。理想的狀態是 **`reward` 曲線上升，同時 `cost` 曲線下降**。


### 關於torques的episode_cost，圖表曲線往上升的意義是甚麼?
這張 `cost/torques` 圖表非常有趣，它揭示了您機器人學習策略的一個重要面向。讓我為您詳細解讀。

### 圖表曲線往上升的直接意義

首先，我們回顧一下 `cost` 的概念：它是您**不希望**看到的行為，數值越低越好。

`cost/torques` 這個指標是用來懲罰機器人使用過大的馬達力矩（Torque）。因此，這張圖表的曲線往上升，代表的直接意義是：

**隨著訓練的進行，您的機器人學會了使用越來越大、越來越強的馬達力矩來完成任務。**

換句話說，它正在學習一種更「暴力」、更「積極」的移動策略。

### 為什麼會這樣？這到底是好是壞？

這本身不完全是壞事，但它揭示了強化學習中一個非常核心的概念：**權衡與取捨 (Trade-off)**。

#### 1\. 為什麼機器人要選擇高力矩？

機器人的最終目標是最大化**總獎勵 (`total_reward`)**，而總獎勵是由多個部分組成的：

`總獎勵 = (追蹤速度獎勵 + 存活獎勵 + ...) - (力矩成本 + 動作平滑度成本 + ...)`

在訓練初期（圖表左側），機器人可能還在學習如何保持平衡，動作幅度小，所以力矩成本很低。

隨著訓練推進，機器人發現，如果它敢於做出**更快速、更有力的動作**（這必然會增加力矩），它就能**更精準地追蹤目標速度**（`tracking_lin_vel` 和 `tracking_ang_vel`）。而從獎勵權重來看，追蹤速度帶來的獎勵（權重為 `2.5` 和 `6.0`）遠遠高於力矩帶來的懲罰（權重僅為 `-0.001`）。

因此，機器人「計算」出：**花費一點力矩成本，去換取高額的速度獎勵，是一筆划算的交易**。這就是為什麼 `cost/torques` 曲線會隨著 `eval/episode_reward` 曲線（您上一張圖）一起上升的原因。

#### 2\. 這到底是好是壞？

這取決於您的最終目標：

  * **好的方面**：這表明您的機器人正在學會一種**動態且主動的控制策略**。它不再是小心翼翼地移動，而是變得更加「自信」，能夠利用馬達的力量來快速達成目標。對於需要敏捷反應的任務來說，這是一個好的趨勢。

  * **潛在的壞處**：

      * **能源效率低**：高力矩意味著高耗電量。如果這是一個需要長時間運作的真實機器人，這種策略可能會很快耗盡電池。
      * **硬體損耗**：長期高強度的力矩輸出可能會加速馬達和齒輪箱的磨損。
      * **動作可能不自然**：有時候，過於激進的策略可能會導致機器人動作看起來很僵硬或充滿了急加減速，不夠流暢。

### 結論與下一步建議

**結論**：`cost/torques` 曲線上升，說明您的機器人為了最大化整體獎勵，學會了犧牲能源效率（增加力矩成本）來換取更高的任務表現（追蹤速度獎勵）。考慮到您的總獎勵 `episode_reward` 在持續上升，可以說**在目前的獎勵設定下，這是一個成功的學習策略**。

**下一步建議**：

如果您覺得目前機器人的動作太「暴力」，希望它變得更節能、更柔順，您可以動手調整獎勵權重：

1.  **打開檔案**：`playground/open_duck_mini_v2/joystick.py`
2.  **找到 `reward_config`**。
3.  **增加 `torques` 的懲罰權重**。例如，將 `-1.0e-3` (-0.001) 修改成一個絕對值更大的數字，比如 `-5.0e-3` (-0.005) 或 `-1.0e-2` (-0.01)。
    ```python
    # in joystick.py
    'torques': -5.0e-3,  # 增加懲罰
    ```
4.  **重新訓練**：用新的權重進行訓練，您應該會在 TensorBoard 中看到 `cost/torques` 的上升趨勢變得平緩，甚至可能下降。

這就是強化學習的魅力所在——透過調整獎勵函數，您可以像訓練寵物一樣，塑造您的機器人，讓它學會您期望的行為模式。


### 要如何判讀最佳的最大訓練步數?
這是一個非常專業且核心的問題。在強化學習中，並不存在一個適用於所有情況的「最佳」訓練步數。正確的做法是**透過觀察訓練過程中的各項指標，來判斷模型何時達到「收斂 (Convergence)」狀態**，這時就是最理想的停止點。

您用來判斷的主要工具就是 **TensorBoard**。

### 用來判斷的核心指標

您需要同時觀察以下幾個關鍵圖表，來綜合判斷訓練的狀態：

1.  **`eval/episode_reward` (最重要)**：平均單集獎勵。
2.  **`episode_cost`**：平均單集成本 (例如 `cost/torques` 等的總和)。
3.  **`reward_std`**：獎勵的標準差，代表穩定性。

### 如何判讀並決定最佳停止點

以下是幾種常見的情況，以及它們代表的意義：

#### 情況一：理想的收斂 (Ideal Convergence) - **最佳停止點**

  * **圖表特徵**：

      * `eval/episode_reward` 曲線在經過一段時間的快速上升後，進入一個**平穩的高原期 (plateau)**，不再有明顯的增長。
      * `episode_cost` 曲線也相對應地下降到一個穩定值。
      * `reward_std` 穩定在一個相對較小的值。

  * **意義**：
    這表示模型已經學到了在當前環境和獎勵設定下所能學到的絕大部分知識。繼續訓練下去，它也很難再有突破性的進步。

  * **判斷**：
    當 `reward` 曲線進入高原期並持續一段相當長的時間（例如，幾千萬步）都沒有顯著變化時，就可以認定模型已經收斂。**這個高原期的起點，就是您要找的最佳訓練區間**。再繼續訓練的成本很高，但收益極低。

  * **對應您的圖表**：
    您的 `eval/episode_reward` 圖表就是一個非常典型的理想收斂案例。在大約 **200M (2億) 步**之後，獎勵值就進入了一個穩定的高原期。因此，對您來說，將訓練步數設定在 **200M 到 300M 之間**都是非常合理的。

#### 情況二：邊際效益遞減 (Diminishing Returns)

  * **圖表特徵**：

      * `reward` 曲線還在非常緩慢地上升，但上升的斜率已經變得極度平緩。

  * **意義**：
    模型仍在學習，但學習速度變得非常慢。可能再訓練一億步，`reward` 也只會增加一點點。

  * **判斷**：
    這是一個權衡點。您需要判斷，為了這一點點微小的提升，是否值得花費大量的時間和計算資源。在很多情況下，這個時候也可以考慮停止訓練。

#### 情況三：不穩定或過擬合 (Instability / Overfitting)

  * **圖表特徵**：

      * `reward` 曲線在達到一個峰值後，開始**持續下降**，或者出現劇烈的、大幅度的震盪。

  * **意義**：
    這是一個危險的信號。可能的原因是學習率沒有衰減，導致模型在後期開始「忘記」之前學到的東西，或者過度擬合了某些特定的隨機情況，導致泛化能力下降。

  * **判斷**：
    **應立即停止訓練！** 這種情況下，最佳的模型不是訓練到最後的模型，而是儲存在**獎勵曲線峰值點**的那個 checkpoint。

### 總結與實務流程

1.  **開始一個足夠長的訓練**：像您一樣設定一個較大的 `num_timesteps` (例如 300M 或 500M) 是一個好習慣。
2.  **使用 TensorBoard 監控**：在訓練過程中，持續觀察 `eval/episode_reward` 的曲線。
3.  **尋找高原期**：當您看到獎勵曲線開始變平，進入收斂狀態時，就找到了最佳的訓練區間。
4.  **選擇模型**：從這個高原期區間內儲存的 checkpoints (`.onnx` 檔案) 中，挑選一個 `reward` 數值高且穩定的模型作為您的最終成果。

根據您的圖表，**您將訓練步數設定在 3 億步是完全正確的**，因為它完整地涵蓋了從學習到收斂的整個過程，並讓您能夠從中選出最佳的模型。


### 訓練綜合地形時的觀念

這張圖表顯示「沒有任何規律」和劇烈波動，恰恰說明了您的訓練遇到了一個新的、更困難的挑戰。

這**不是程式錯誤**，而是一個典型的強化學習現象：當任務難度大幅增加時，模型很難找到一個有效的學習路徑。

讓我為您詳細解釋這張圖表的意義，以及為什麼會這樣，最後提供解決這個問題的專業方法。

### 圖表意義詳解

  * **劇烈波動的曲線**：您看到的 `eval/episode_reward` 曲線在高值和低值之間劇烈跳動，不像之前那樣平滑上升。
  * **低迷的平滑趨勢線**：中間那條深色的平滑趨勢線，幾乎是一條沒有上升的水平線，並且一直維持在一個非常低的分數（大約 20-40 分）。

這兩點共同說明了：**您的機器人模型沒有在學習中取得任何進展**。它陷入了一種困境：有時候運氣好，它可能在平地上走得不錯，拿到一些分數；但更多時候，它一遇到崎嶇地面就立刻失敗（摔倒），導致分數極低。最終，平均下來的總獎勵就變得又低又混亂。

### 為什麼在綜合地形上訓練會失敗？

1.  **任務難度指數級增長**：
    讓機器人同時學會「在平地行走」和「在崎嶇地行走」這兩種技能，其難度**遠大於**只學習其中一種。模型需要找到一套能夠適應兩種完全不同環境的通用策略，這對神經網路來說是一個巨大的挑戰。

2.  **「平均」的陷阱**：
    您在 TensorBoard 上看到的是**平均獎勵**。因為您的 `reset` 函式會隨機將機器人放在平地或崎嶇地，所以這個平均值混合了兩種情況：

      * **在平地上的表現 (可能還不錯)**
      * **在崎嶇地上的表現 (極有可能立刻失敗)**
        只要機器人還沒學會如何在崎嶇地面上生存，那麼大量的「零分」表現就會拉低整體的平均分，並造成劇烈的波動。

3.  **探索與懲罰的困境**：
    機器人可能嘗試了幾次在崎嶇地面上行走，但每次都很快摔倒。摔倒會導致它失去未來所有可能的 `alive` 獎勵，這是一個巨大的懲罰。幾次嘗試後，模型可能會學到一個「錯誤的結論」：「崎嶇地面太危險了，最好的策略就是盡量避免在那裡做任何大幅度動作」，這反而阻礙了它去探索和學習正確的策略。

### 如何解決：引入「課程學習 (Curriculum Learning)」

要解決這個問題，最有效、最專業的方法是採用**課程學習**。這個概念就像教小孩子一樣：我們不會讓他直接去考大學，而是先從國小、國中、高中一步步教起。

我們對機器人的訓練也要遵循這個原則。

#### 步驟一：使用「平地專家」模型作為起點 (國小畢業)

您之前在 `flat_terrain` 上已經訓練出了一個非常成功的模型。這個模型已經是「平地行走專家」了。我們要把它的知識傳承下來。

1.  **找到您最好的平地模型**：從 `my_checkpoints` 資料夾中，找到您之前在平地訓練時，`reward` 分數最高的那個模型。例如，我們之前提到的 `2025_09_09_231820_279019520.onnx`。
2.  **找到對應的模型權重資料夾**：與 `.onnx` 檔案對應的，是一個儲存了 JAX 完整權重的資料夾。它的名字應該就是 `2025_09_09_231820_279019520`。

#### 步驟二：在新任務中載入舊權重進行「進階訓練」(上高中)

現在，我們要開始新的 `combined_terrain_backlash` 任務訓練，但不是從零開始，而是站在「平地專家」的肩膀上。

請修改您的訓練指令，加入 `--restore_checkpoint_path` 參數，指向您找到的最佳模型的權重資料夾：

```bash
uv run playground/open_duck_mini_v2/runner.py \
--env joystick \
--task combined_terrain_backlash \
--output_dir my_combined_checkpoints_finetune \
--num_timesteps 1500000000 \
--restore_checkpoint_path my_checkpoints/2025_09_09_231820_279019520 
```

**指令解釋**：

  * `--task combined_terrain_backlash`: 我們仍然使用新的、困難的任務。
  * `--output_dir my_combined_checkpoints_finetune`: 建議使用一個新的資料夾來儲存這次的訓練結果，以便區分。
  * `--restore_checkpoint_path my_checkpoints/2025_09_09_231820_279019520`: **這是最關鍵的一步**。這個參數會讓訓練程式在開始時，先載入您指定的舊模型權重，而不是從隨機權重開始。

### 預期結果

採用課程學習後，您的訓練將會發生質的改變：

  * **更高的起點**：您會看到 TensorBoard 上的 `reward` 曲線一開始就在一個比較高的位置，而不是從零開始。
  * **更穩定的學習**：因為機器人已經掌握了平地行走的基礎，它只需要專注於學習「如何在崎嶇地面上調整步伐以避免摔倒」這個新技能。學習過程會變得更穩定，`reward` 曲線也更有可能呈現出平穩的上升趨勢。

這是解決複雜強化學習任務時非常強大且常用的技巧。請嘗試這個方法，觀察一下訓練曲線的變化。


### 麼從 my_checkpoints 資料夾中找到最好的平地模型
從 `my_checkpoints` 資料夾中找到最好的平地模型，有兩種方法：一種是透過**文字 LOG**，另一種是使用 **TensorBoard** 這個更直觀的工具。

### 方法一：從訓練 LOG 中手動尋找 (您目前可用的方法)

這是最直接的方法。您需要仔細查看您先前貼出的、在平地 (`flat_terrain`) 上訓練的 LOG 檔案。

#### 步驟 1: 理解 checkpoint 的命名規則

首先，要理解程式儲存模型的方式。每一次儲存，程式都會做兩件事：

1.  建立一個資料夾，儲存 JAX 格式的完整權重。
2.  建立一個 `.onnx` 檔案，用於推論。

它們的命名規則是：**`[日期]_[時間]_[當前訓練步數]`**

例如，您 LOG 中的這一行：
`Saving checkpoint (step: 279019520): /.../my_checkpoints/2025_09_09_231820_279019520`

這表示，在訓練進行到 `279,019,520` 步時，程式儲存了名為 `2025_09_09_231820_279019520` 的 JAX 權重資料夾，以及一個同名的 `.onnx` 檔案。

#### 步驟 2: 掃描 LOG，找出最高獎勵 (Reward)

現在，請回顧您貼出的兩份 LOG，我們來找出哪一次訓練的哪個步驟表現最好。

**第一次訓練 (從 `22:26` 開始)**

  * `STEP: 193167360`, `reward: 294.89`
  * `STEP: 279019520`, **`reward: 299.31` (這次訓練的最高點)**
  * `STEP: 300482560`, `reward: 252.90` (分數開始下降)

**第二次訓練 (從 `05:35` 開始)**

  * `STEP: 171704320`, `reward: 281.16`
  * `STEP: 193167360`, **`reward: 303.68` (兩次訓練中的最高點)**
  * `STEP: 214630400`, `reward: 265.84` (分數開始下降)

#### 步驟 3: 鎖定最佳模型檔案

從上面的分析我們可以得出結論：

**在您提供的所有 LOG 中，表現最好的模型是在第二次訓練的第 `193,167,360` 步產生的，當時的平均獎勵達到了 `303.68`。**

根據命名規則，這個最佳模型對應的檔案就是：

  * **JAX 權重資料夾 (用於恢復訓練)**: `my_checkpoints/2025_09_10_061358_193167360`
  * **ONNX 推論檔案 (用於 mujoco\_infer.py)**: `my_checkpoints/2025_09_10_061358_193167360.onnx`

-----

### 方法二：使用 TensorBoard 視覺化尋找 (建議的最佳實踐)

雖然從文字 LOG 中尋找是可行的，但當訓練時間更長、儲存點更多時，這種方法會變得很繁瑣。使用 TensorBoard 是更專業、更高效的方式。

1.  **啟動 TensorBoard**：
    在您的專案根目錄下，開啟一個新的終端機，並執行：

    ```bash
    uv run tensorboard --logdir=my_checkpoints
    ```

      * `logdir=my_checkpoints` 會告訴 TensorBoard 去掃描 `my_checkpoints` 資料夾下的所有訓練紀錄。

2.  **打開網頁**：
    終端機上會顯示一個網址，通常是 `http://localhost:6006/`。將它複製到您的瀏覽器中打開。

3.  **尋找峰值**：
    在 TensorBoard 的介面中，找到 `eval/episode_reward` 這張圖表。您可以用滑鼠在曲線上移動，圖表會顯示出每個數據點的具體 `Step` 和 `Value` (獎勵值)。您只需要找到曲線的**最高點 (峰值)**，記下那個點的 `Step` 數。

    *(這是一張示意圖，展示如何在曲線上找到峰值)*

4.  **對應檔案**：
    根據您在 TensorBoard 上找到的最佳 `Step` 數，回到 `my_checkpoints` 資料夾中，找到對應的 checkpoint 檔案即可。

**總結：** 透過分析您提供的 LOG，**您應該使用的最佳平地模型是 `my_checkpoints/2025_09_10_061358_193167360`**。