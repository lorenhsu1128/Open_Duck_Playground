### 專案總覽

這個專案是一個使用 [MuJoCo](https://mujoco.org/) 物理模擬器和 [Brax](https://github.com/google/brax) 函式庫，對名為 "Open Duck Mini V2" 的雙足機器人進行強化學習（Reinforcement Learning）訓練的框架。其主要目標是訓練機器人學習執行不同的任務，例如：根據指令行走（`joystick` 任務）以及維持站立平衡（`standing` 任務）。

此專案包含了完整的機器人模型（XML 檔）、訓練環境設置、獎勵函數、訓練流程、以及將訓練好的模型匯出並進行推論（inference）的工具。

好的，這沒有問題。我已經詳細閱讀了您提供的所有檔案，這是一個結構非常清晰的強化學習專案。

以下是每個檔案與資料夾的用途詳解：

### **專案根目錄**

* **`.gitignore`**: Git 版本控制的設定檔，告訴 Git 哪些檔案或資料夾（例如 `__pycache__/`、`.DS_Store`）不需要被追蹤。
* **`README.md` / `README_LOREN.md`**: 專案說明文件。`README.md` 通常是原始的說明，而 `README_LOREN.md` 是一份更詳細、客製化的版本，解釋了專案的總覽、檔案功能和如何執行。
* **`pyproject.toml`**: Python 專案的標準設定檔。它定義了專案的依賴套件（如 `brax`, `mujoco`, `jax`）和建置工具（`uv`），讓您可以方便地建立虛擬環境並安裝所有必要的函式庫。

---

### **`playground/common/` (共用程式碼)**

這個資料夾包含了整個專案通用的、可重複使用的程式碼，不針對任何特定的機器人。

* **`runner.py`**: 定義了 `BaseRunner` 類別，是整個訓練流程的核心框架。它負責初始化 PPO (Proximal Policy Optimization) 演算法、設定訓練參數、啟動訓練循環、以及處理模型的儲存和記錄。
* **`export_onnx.py`**: 提供將 JAX/Flax 格式的訓練模型轉換為 ONNX (Open Neural Network Exchange) 格式的工具。ONNX 是一個開放標準，讓模型可以在不同平台和框架之間使用。
* **`onnx_infer.py`**: 提供 `OnnxInfer` 類別，用於載入 ONNX 模型並進行推論（inference）。`mujoco_infer.py` 會使用它來讀取訓練好的模型。
* **`plot_saved_obs.py`**: 一個實用腳本，用於讀取在推論時儲存的 `mujoco_saved_obs.pkl` 檔案，並使用 `matplotlib` 將機器人的觀測數據（如速度、關節角度）繪製成圖表，方便分析。
* **`poly_reference_motion.py` / `poly_reference_motion_numpy.py`**: 用於處理和計算「參考動作」的類別。它們會讀取 `polynomial_coefficients.pkl` 檔案，並根據給定的指令計算出機器人應該模仿的目標關節姿態。一個使用 JAX (用於訓練)，另一個使用 NumPy。
* **`randomize.py`**: 實現「領域隨機化 (Domain Randomization)」功能。在訓練過程中，它會隨機調整模擬環境的物理參數（如摩擦力、質量），以增強訓練出模型的泛化能力，使其更能適應真實世界。
* **`rewards.py` / `rewards_numpy.py`**: 包含一系列標準的獎勵/懲罰函數，例如追蹤速度的獎勵、消耗力矩的懲罰、維持存活的獎勵等。
* **`utils.py`**: 包含一些通用的工具函式，例如用於平滑化輸出動作的低通濾波器 `LowPassActionFilter`。

---

### **`playground/open_duck_mini_v2/` (機器人專屬程式碼)**

這個資料夾包含了所有專為 "Open Duck Mini V2" 這款雙足機器人設計的程式碼。

* **`base.py`**: 定義了 `OpenDuckMiniV2Env` 這個基礎環境類別，所有該機器人的特定任務都會繼承它。它負責載入 MuJoCo 模型和處理基本的機器人狀態讀取。
* **`constants.py`**: 存放專為此機器人設計的常數，例如 XML 模型的路徑、身體部位的名稱等，避免在程式碼中寫死字串。
* **`custom_rewards.py` / `custom_rewards_numpy.py`**: 實現此機器人專屬的客製化獎勵函數，例如模仿參考動作的 `reward_imitation`。
* **`joystick.py`**: 定義「搖桿 (joystick)」任務的環境。目標是訓練機器人根據隨機生成的速度指令來行走。它組合了 `common/rewards.py` 中的獎勵函數來形成這個任務的總獎勵。
* **`standing.py`**: 定義「站立 (standing)」任務的環境。目標是訓練機器人在受到外力干擾時仍能保持平衡。
* **`mujoco_infer.py`**: **執行即時推論的腳本**。它會載入一個訓練好的 `.onnx` 模型，並打開一個 MuJoCo 互動視窗，讓您可以用鍵盤控制機器人並觀察其表現。
* **`mujoco_infer_base.py`**: `mujoco_infer.py` 的基礎類別，處理底層的模型載入和狀態更新。
* **`ref_motion_viewer.py`**: 一個視覺化工具，用於在 MuJoCo 視窗中單獨觀察 `polynomial_coefficients.pkl` 中定義的參考動作，方便偵錯。
* **`runner.py`**: **啟動訓練的主要腳本**。它會解析命令列參數（例如 `--env joystick`），然後呼叫 `common/runner.py` 中的框架來開始針對此機器人的訓練。
* **`data/polynomial_coefficients.pkl`**: 儲存了用多項式擬合的步態資料，作為機器人模仿學習的目標。

---

### **`playground/open_duck_mini_v2/xmls/` (MuJoCo 模擬模型)**

這個資料夾是整個模擬專案的**物理核心**，定義了機器人和它所在的世界。

* **`open_duck_mini_v2.xml`**: **核心機器人模型檔**。用 XML 格式詳細描述了機器人的所有連桿 (`body`)、關節 (`joint`)、質量 (`inertial`)、致動器 (`actuator`) 和 3D 視覺模型 (`geom`)。
* **`open_duck_mini_v2_backlash.xml`**: 上述模型的一個特殊版本，額外加入了「背隙 (backlash)」關節，用來模仿真實世界中齒輪之間的微小間隙，讓模擬更貼近現實。
* **`scene_*.xml` (場景檔案)**: 這些檔案定義了「世界」。它們會透過 `<include>` 標籤將機器人模型載入，並將其放置在一個特定的環境中。
    * `scene_flat_terrain.xml`: 包含一個無限大的平坦地面。
    * `scene_rough_terrain_backlash.xml`: 建立一個崎嶇不平的地面。
    * `scene_combined_terrain_backlash.xml`: 建立一個同時包含平地和崎嶇地的混合場景。
* **`joints_properties.xml` / `sensors.xml`**: 模組化的設定檔，分別定義了所有關節馬達的通用屬性和所有感測器的屬性，被主模型檔案引用，使結構更清晰。
* **`config.json`**: 一個 JSON 格式的設定檔。
* **`assets/` (資產資料夾)**:
    * **`*.stl` / `*.part`**: 機器人各個零件的 3D 模型檔案，用於在模擬中視覺化呈現。
    * **`hfield.png`**: 一張灰階圖片，作為高度圖，用來生成崎嶇不平的地形。

### PY 檔案功能詳解

以下是每個 `.py` 檔案的功能說明：

#### `playground/open_duck_mini_v2/` (機器人專屬程式碼)

  * **`base.py`**:

      * **功能**: 定義了 `OpenDuckMiniV2Env` 這個基礎環境類別，所有該機器人的特定任務（task）都會繼承這個類別。它處理 MuJoCo 模型的載入、讀取機器人關節和致動器（actuator）的資訊、以及提供讀取/設定機器人狀態（如位置、速度）的輔助函式。

  * **`constants.py`**:

      * **功能**: 存放專案中會用到的常數，例如 XML 模型的檔案路徑、機器人特定部位（如腳、軀幹）的名稱、以及感測器的名稱。這樣可以方便統一管理，避免在程式碼中寫死（hard-code）字串或數值。

  * **`custom_rewards.py`**:

      * **功能**: 實現了名為 `reward_imitation` 的客製化獎勵函數。這個函數的目的是讓機器人模仿一個預先定義好的參考動作（reference motion），透過比較機器人目前的動作與參考動作的差異來給予獎勵，藉此學習更自然的步態。

  * **`joystick.py`**:

      * **功能**: 定義了「搖桿（joystick）」任務的環境。在這個任務中，機器人需要學習根據一個隨機生成的命令（包含前進/後退、左/右平移、旋轉的速度）來移動。這個檔案也定義了這個任務專屬的觀察（observation）空間、獎勵函數（例如：追蹤目標速度、站立姿態、存活獎勵等）以及訓練結束的條件。

  * **`standing.py`**:

      * **功能**: 定義了「站立（standing）」任務的環境。這個任務的目標是訓練機器人在受到外部推力干擾的情況下，仍能保持平衡並站立不動。

  * **`mujoco_infer.py`**:

      * **功能**: 讓使用者可以在 MuJoCo 的互動式視窗中，載入一個訓練好的 ONNX 模型，並即時觀察機器人的推論結果。使用者可以透過鍵盤輸入來控制機器人的移動命令。

  * **`mujoco_infer_base.py`**:

      * **功能**: `mujoco_infer.py` 的基礎類別，負責處理 MuJoCo 模型的載入和機器人狀態的讀取，讓 `mujoco_infer.py` 可以專注在推論的邏輯上。

  * **`ref_motion_viewer.py`**:

      * **功能**: 一個用來視覺化參考動作的工具。它可以載入 `polynomial_coefficients.pkl` 檔案，並在 MuJoCo 視窗中顯示出機器人模仿該動作的樣子，方便開發者調試或觀察參考動作是否正確。

  * **`runner.py`**:

      * **功能**: `open_duck_mini_v2` 機器人的主要訓練腳本。它會根據命令列參數來選擇要執行的任務（`joystick` 或 `standing`），並啟動 `playground/common/runner.py` 中的訓練流程。

#### `playground/common/` (共用程式碼)

  * **`runner.py`**:

      * **功能**: 定義了 `BaseRunner` 類別，這是一個通用的訓練流程框架。它負責初始化環境、設定 PPO 訓練演算法的參數、啟動訓練、以及處理訓練過程中的回呼（callback）函式（例如：儲存模型、記錄訓練數據到 TensorBoard）。

  * **`export_onnx.py`**:

      * **功能**: 提供 `export_onnx` 函式，可以將 JAX/Flax 訓練出的模型權重，轉換並儲存成 ONNX (Open Neural Network Exchange) 格式。ONNX 是一個開放的模型格式，讓模型可以在不同的深度學習框架之間轉移。

  * **`onnx_infer.py`**:

      * **功能**: 提供 `OnnxInfer` 類別，方便載入 ONNX 模型並進行推論。`mujoco_infer.py` 就是使用這個類別來讀取模型。

  * **`plot_saved_obs.py`**:

      * **功能**: 一個用來繪製並分析儲存的觀察（observation）數據的腳本。當 `mujoco_infer.py` 執行時，會將每一步的觀察數據儲存成 `mujoco_saved_obs.pkl`，這個腳本可以讀取該檔案並用 `matplotlib` 繪製成圖表，方便分析機器人的狀態變化。

  * **`poly_reference_motion.py` / `poly_reference_motion_numpy.py`**:

      * **功能**: 這兩個檔案的功能相似，都是用來讀取並處理 `polynomial_coefficients.pkl` 參考動作檔。這個檔案儲存了用多項式擬合（polynomial fitting）的步態資料。`PolyReferenceMotion` 類別可以根據給定的移動指令（速度）和時間點，計算出對應的參考關節角度、速度等資訊。`_numpy` 版本是使用 NumPy，而另一個版本則是使用 JAX 的 `jax.numpy`。

  * **`randomize.py`**:

      * **功能**: 實現了「領域隨機化（Domain Randomization）」的功能。在訓練過程中，它會隨機改變模擬環境中的一些物理參數，例如：地板摩擦力、機器人各部位的質量、馬達的力臂等。這樣做可以讓訓練出來的模型更具備泛化能力（generalization），更能適應真實世界的物理變化。

  * **`rewards.py` / `rewards_numpy.py`**:

      * **功能**: 提供了一系列常用的獎勵/懲罰函數（reward/cost functions），例如：追蹤速度的獎勵、維持特定方向的懲罰、力矩（torque）消耗的懲罰、動作變化率的懲罰、維持存活的獎勵等等。`joystick.py` 和 `standing.py` 會根據任務需求來組合這些函數。

  * **`utils.py`**:

      * **功能**: 包含一些工具函式，例如 `LowPassActionFilter`。這是一個低通濾波器，可以用來平滑化神經網路輸出的動作指令，讓機器人的動作看起來更穩定、更不容易抖動。

### 命令列執行參數

以下是主要幾個可以執行的 `.py` 檔案以及它們的命令列參數：

#### 1\. `playground/open_duck_mini_v2/runner.py` (訓練)

  * **用途**: 啟動訓練流程。
  * **命令範例**:
    ```bash
    python playground/open_duck_mini_v2/runner.py --env joystick --task flat_terrain --output_dir my_checkpoints
    ```
  * **參數**:
      * `--output_dir`: 設定儲存模型權重（checkpoints）和 TensorBoard 紀錄檔的資料夾路徑。預設為 `"checkpoints"`。
      * `--num_timesteps`: 訓練的總時間步數（timesteps）。預設為 `150,000,000`。
      * `--env`: 選擇要訓練的環境。可選 `joystick` 或 `standing`。預設為 `"joystick"`。
      * `--task`: 選擇任務的場景。例如 `flat_terrain` (平坦地面) 或 `rough_terrain` (崎嶇地面)。預設為 `"flat_terrain"`。
      * `--restore_checkpoint_path`: 如果要從一個已經儲存的模型權重繼續訓練，可以指定該權重檔案的路徑。預設為 `None`。

#### 2\. `playground/open_duck_mini_v2/mujoco_infer.py` (推論)

  * **用途**: 使用 MuJoCo 互動視窗進行即時推論。
  * **命令範例**:
    ```bash
    python playground/open_duck_mini_v2/mujoco_infer.py -o path/to/your/model.onnx
    ```
  * **參數**:
      * `-o`, `--onnx_model_path`: **(必要)** 指定要載入的 ONNX 模型檔案路徑。
      * `--reference_data`: 參考動作資料 `polynomial_coefficients.pkl` 的路徑。預設為 `"playground/open_duck_mini_v2/data/polynomial_coefficients.pkl"`。
      * `--model_path`: MuJoCo 場景 XML 檔案的路徑。預設為 `"playground/open_duck_mini_v2/xmls/scene_flat_terrain.xml"`。
      * `--standing`: 如果設定了這個旗標，會進入站立模式。預設為 `False`。

#### 3\. `playground/open_duck_mini_v2/ref_motion_viewer.py` (參考動作觀察)

  * **用途**: 在 MuJoCo 中觀看參考動作。
  * **命令範例**:
    ```bash
    python playground/open_duck_mini_v2/ref_motion_viewer.py --command 0.1 0.0 0.0
    ```
  * **參數**:
      * `--reference_data`: 參考動作資料 `polynomial_coefficients.pkl` 的路徑。預設為 `"playground/go_bdx/data/polynomial_coefficients.pkl"`。
      * `-joystick`: 如果設定了這個旗標，並且電腦有連接搖桿，則可以使用搖桿來控制命令。
      * `--command`: 以三個浮點數設定參考動作的命令 `[dx, dy, dtheta]` (X軸速度, Y軸速度, 旋轉速度)。預設為 `[0.0, -0.05, -0.1]`。
      * `--scene`: 選擇要載入的場景。預設為 `"flat_terrain"`。

#### 4\. `playground/common/plot_saved_obs.py` (繪製觀察數據)

  * **用途**: 繪製 `mujoco_infer.py` 執行後儲存的 `mujoco_saved_obs.pkl`。
  * **命令範例**:
    ```bash
    python playground/common/plot_saved_obs.py -d mujoco_saved_obs.pkl
    ```
  * **參數**:
      * `-d`, `--data`: 指定要讀取的 `.pkl` 檔案路徑。預設為 `"mujoco_saved_obs.pkl"`。


### 透過修改命令列參數和程式碼中的特定檔案來增加訓練量與調整隨機訓練參數。

### 1\. 增加總訓練量

增加總訓練量最直接的方法是修改訓練腳本的命令列參數，或是直接調整 PPO 演算法的超參數。

#### 方法一：透過命令列參數 (建議)

在執行訓練時，您可以直接調整 `--num_timesteps` 參數來設定訓練的總步數。總步數越多，代表機器人與環境互動的次數越多，也就是訓練量越大。

  * **檔案**: `playground/open_duck_mini_v2/runner.py`
  * **參數**: `--num_timesteps`
  * **說明**: 這個參數定義了整個訓練過程將執行的環境互動總步數。預設值是 `150,000,000`。
  * **範例**: 將訓練總量增加到 3 億步。
    ```bash
    uv run playground/open_duck_mini_v2/runner.py --task flat_terrain --num_timesteps 300000000
    ```

#### 方法二：修改 PPO 超參數

如果您想更細緻地控制每一輪訓練的資料量，可以修改 PPO 演算法的參數，特別是 `num_envs`。

  * **檔案**: `playground/common/runner.py`
  * **參數**: `num_envs`
  * **說明**: 這個參數設定了要**同時平行運行的模擬環境數量**。越多的平行環境代表在同一個時間單位內可以收集到越多的訓練數據，進而增加訓練量，但也會消耗更多的 GPU 記憶體。預設值是 `8192`。
  * **修改位置**: 在 `playground/common/runner.py` 檔案中，找到 `locomotion_params.brax_ppo_config` 這一行。您可以覆寫其預設值，例如：
    ```python
    # in playground/common/runner.py

    def train(self) -> None:
        self.ppo_params = locomotion_params.brax_ppo_config(
            "BerkeleyHumanoidJoystickFlatTerrain"
        )
        self.ppo_training_params = dict(self.ppo_params)
        
        # 在這裡增加或修改參數
        self.ppo_training_params["num_envs"] = 16384  # 例如，將平行環境數量加倍
        
        # ... 後續程式碼 ...
    ```

-----

### 2\. 調整隨機訓練參數 (Domain Randomization)

「領域隨機化」是在訓練過程中隨機改變模擬器的物理參數，目的是為了讓訓練出的模型更具備泛化能力，更能適應真實世界的變化。所有的隨機化邏輯都定義在 `domain_randomize` 函式中。

  * **檔案**: `playground/common/randomize.py`
  * **函式**: `domain_randomize`

您可以直接修改此檔案中的 `minval` 和 `maxval` 來調整各項參數的隨機範圍。以下是可供調整的主要參數：

  * **地板摩擦力 (Floor Friction)**

      * **用途**: 模擬不同材質的地板。
      * **程式碼**:
        ```python
        geom_friction = model.geom_friction.at[FLOOR_GEOM_ID, 0].set(
            jax.random.uniform(key, minval=0.5, maxval=1.0)  # 原範圍是 0.4 到 1.0
        )
        ```

  * **關節靜摩擦力 (Static Friction)**

      * **用途**: 模擬關節的阻力變化。
      * **程式碼**:
        ```python
        frictionloss = model.dof_frictionloss[dof_addr] * jax.random.uniform(
            key, shape=(model.nu,), minval=0.9, maxval=1.1
        )
        ```

  * **馬達電樞 (Motor Armature)**

      * **用途**: 模擬馬達內部電阻或慣性的變化。
      * **程式碼**:
        ```python
        armature = model.dof_armature[dof_addr] * jax.random.uniform(
            key, shape=(model.nu,), minval=1.0, maxval=1.05
        )
        ```

  * **質心位置 (Center of Mass Position)**

      * **用途**: 模擬機器人重心因組裝或負重造成的偏移。
      * **程式碼**:
        ```python
        dpos = jax.random.uniform(key, (3,), minval=-0.05, maxval=0.05)
        body_ipos = model.body_ipos.at[TORSO_BODY_ID].set(
            model.body_ipos[TORSO_BODY_ID] + dpos
        )
        ```

  * **連桿質量 (Link Masses)**

      * **用途**: 模擬機器人各部位零件的製造公差。
      * **程式碼**:
        ```python
        dmass = jax.random.uniform(key, shape=(model.nbody,), minval=0.9, maxval=1.1)
        body_mass = model.body_mass.at[:].set(model.body_mass * dmass)
        ```

  * **軀幹額外質量 (Added Torso Mass)**

      * **用途**: 模擬機器人背負不同重量的物體。
      * **程式碼**:
        ```python
        dmass = jax.random.uniform(key, minval=-0.1, maxval=0.1)  # 原範圍是 -0.2 到 0.2
        body_mass = body_mass.at[TORSO_BODY_ID].set(body_mass[TORSO_BODY_ID] + dmass)
        ```

  * **初始關節角度 (Initial Joint Positions)**

      * **用途**: 讓機器人從一個稍微不同的初始站姿開始。
      * **程式碼**:
        ```python
        qpos0 = qpos0.at[joint_addr].set(
            qpos0[joint_addr]
            + jax.random.uniform(
                key, shape=(model.nu,), minval=-0.03, maxval=0.03
            )  # 原範圍是 -0.05 到 0.05
        )
        ```

  * **馬達增益 (Motor KP Gain)**

      * **用途**: 模擬馬達控制器的力度差異。
      * **程式碼**:
        ```python
        factor = jax.random.uniform(
            key, shape=(model.nu,), minval=0.9, maxval=1.1
        )  # 原範圍是 0.8 到 1.2
        ```

### Domain Randomization 每個參數的範圍上下限應該設定多少
這是一個非常好且關鍵的問題。Domain Randomization 參數範圍的設定直接影響到訓練的穩定性和最終模型（policy）的泛化能力（robustness），也就是所謂的「Sim-to-Real」——從模擬環境轉移到真實世界的效果。

設定這些範圍沒有一個絕對的「正確答案」，它更像是一門藝術，需要根據目標、機器人硬體特性以及不斷的實驗來調整。不過，我可以提供一套設定這些參數的**指導原則**和**建議範圍**。

### 設定範圍的核心原則

1.  **從現實出發 (Anchor in Reality)**：隨機化的範圍應該圍繞著您對真實世界機器人的最佳估計。例如，如果您知道機器人馬達的力矩誤差大約是 ±10%，那麼您的隨機化範圍就應該涵蓋這個區間。

2.  **循序漸進 (Curriculum Learning)**：一開始就使用非常大的隨機範圍，可能會讓學習任務變得過於困難，導致模型無法收斂。一個常見且有效的策略是：

      * **初期**：使用較小的範圍進行訓練，讓模型先學會基本的技能。
      * **中期**：在模型穩定後，逐漸增大隨機化的範圍，強迫模型去適應更大的變化。
      * **後期**：甚至可以根據模型在特定方面的弱點，針對性地加強某幾項參數的隨機化。

3.  **觀察訓練過程**：

      * **如果訓練很不穩定，獎勵值（reward）無法穩定上升**：這通常意味著隨機化的範圍太大了，任務太難。您應該縮小範圍。
      * **如果訓練非常順利，獎勵值很高，但在真實機器人上表現很差**：這就是典型的「Sim-to-Real Gap」，表示您的模擬環境太單一，隨機化範圍太小，沒有涵蓋真實世界的物理變化。您應該擴大範圍。

### 各項參數的建議範圍與說明

以下是針對 `playground/common/randomize.py` 檔案中各個參數的具體建議。我將其分為「**保守起始範圍**」和「**積極泛化範圍**」。建議您從保守範圍開始訓練。

-----

| 參數 | 用途 | 保守起始範圍 | 積極泛化範圍 | 說明 |
| :--- | :--- | :--- | :--- | :--- |
| **地板摩擦力**\<br\>`geom_friction` | 模擬不同材質的地面，如木地板、磁磚、地毯等。 | `[0.5, 1.0]` | `[0.4, 1.5]` | 摩擦係數很少會低於 0.4 或高於 1.5。範圍太大可能會產生不真實的物理行為。目前的 `[0.5, 1.0]` 是一個很好的起點。 |
| **關節靜摩擦力**\<br\>`frictionloss` | 模擬關節內部的阻力、潤滑程度的差異。 | `* [0.9, 1.1]`\<br\>(±10%) | `* [0.7, 1.3]`\<br\>(±30%) | 這個值代表與模型預設值的乘法因子。±10% 的範圍適合初期訓練。如果發現機器人在真實世界中關節比模擬中更「澀」或更「滑」，可以擴大此範圍。 |
| **馬達電樞**\<br\>`armature` | 模擬馬達內部電阻和慣性的變化，影響關節的響應速度和阻尼。 | `* [1.0, 1.05]`\<br\>(+0\~5%) | `* [0.95, 1.1]`\<br\>(±5\~10%) | 電樞值對穩定性影響較大，建議從小範圍開始，且通常是往上加比較符合真實情況（真實馬達有更多非理想效應）。 |
| **質心位置**\<br\>`body_ipos` | 模擬因組裝公差、額外負重（如感測器）導致的重心偏移。 | `± [0.01, 0.01, 0.01]`\<br\>(±1 cm) | `± [0.03, 0.03, 0.02]`\<br\>(X,Y ±3cm, Z ±2cm) | 對於小型雙足機器人，幾公分的重心偏移已經非常顯著。Z 軸（垂直）的偏移可以小一些，因為機器人對垂直重心的變化更敏感。 |
| **連桿質量**\<br\>`body_mass` | 模擬各部位零件的製造公差。 | `* [0.95, 1.05]`\<br\>(±5%) | `* [0.85, 1.15]`\<br\>(±15%) | ±5% 是一個比較貼近真實 3D 列印或零件製造公差的範圍。如果您的機器人結構很堅固，可以保持小範圍；如果結構較鬆散，可以嘗試更大的範圍。 |
| **軀幹額外質量**\<br\>`body_mass` (Torso) | 模擬機器人攜帶不同重量的負載（例如，更大的電池或額外的電腦）。 | `± 0.1` kg\<br\>(±100g) | `± 0.25` kg\<br\>(±250g) | 這個範圍完全取決於您預期機器人會負載多重的物體。目前的 `±0.1` kg 是個合理的開始。 |
| **初始關節角度**\<br\>`qpos0` | 讓機器人從稍微不同的初始姿態開始訓練，避免過擬合到單一站姿。 | `± 0.03` rad\<br\>(約 ±1.7°) | `± 0.08` rad\<br\>(約 ±4.5°) | 角度的擾動不需要太大，幾度的隨機偏移已經能有效增加訓練的泛化性。 |
| **馬達增益 (KP)**\<br\>`actuator_gainprm` | 模擬馬達 PID 控制器中 P (Proportional) 增益的差異。 | `* [0.9, 1.1]`\<br\>(±10%) | `* [0.8, 1.2]`\<br\>(±20%) | 這個參數直接影響馬達的「硬度」和響應。±10% 的範圍可以模擬不同馬達之間的性能差異。±20% 則是一個更具挑戰性的範圍。 |

### 總結與建議流程

1.  **開始**：使用上表中的「**保守起始範圍**」來設定 `playground/common/randomize.py` 檔案。
2.  **訓練**：執行訓練，並使用 TensorBoard 密切觀察 reward 曲線。如果曲線能夠穩定上升，說明模型正在學習。
3.  **評估**：當模型收斂後，將其部署到真實機器人上進行測試（如果有的話），或者在模擬中給予更困難的挑戰（例如，崎嶇地形 `rough_terrain`）。
4.  **迭代**：
      * 如果 Sim-to-Real 效果不佳，逐步向「**積極泛化範圍**」擴大參數，然後重新訓練。
      * 如果在擴大範圍後訓練變得不穩定，可以嘗試稍微縮小範圍，或者增加總訓練量 (`--num_timesteps`)，給模型更多的時間去學習和適應。

透過這樣不斷實驗和調整的過程，您就能找到最適合您機器人和任務的 Domain Randomization 參數設定。

### LOG內容詳細解讀
將會詳細解釋所有LOG內容，並教您如何從中判斷訓練結果的好壞。

### Log 內容逐段詳解

我將以您第一次訓練的 LOG 為例進行說明，因為兩次的結構是完全一樣的。

#### 1\. 啟動與環境設定 (開頭部分)

```
2025-09-09 22:26:04.050337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory...
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1757427964.102760      533 cuda_dnn.cc:8579] Unable to register cuDNN factory...
...
W0000 00:00:1757427964.222262      533 computation_placer.cc:177] computation placer already registered...
```

  * **內容意思**：這部分是 JAX 和 TensorFlow 在初始化時的日誌。這些 `Error (E)` 和 `Warning (W)` 訊息是**正常且無害的**。它們表示程式中的多個部分（JAX 和 TF）都嘗試去註冊同一個 CUDA 函式庫，但因為已經被註冊過了，所以後續的嘗試會跳出警告。這不影響程式的正常運行。

-----

#### 2\. MuJoCo 環境載入

```
xml: /home/loren/bdx/Open_Duck_Playground/playground/open_duck_mini_v2/xmls/scene_flat_terrain.xml
actuators: ['left_hip_yaw', 'left_hip_roll', ...]
joints: ['floating_base', 'left_hip_yaw', ...]
backlash joints: []
...
```

  * **內容意思**：這表示程式成功讀取了 MuJoCo 的模型檔案 (`.xml`)，並且正確解析出機器人的所有致動器（actuators）和關節（joints）資訊。這是訓練開始前的必要準備工作。

-----

#### 3\. PPO 演算法參數設定

```
Observation size: 101
PPO params: {'action_repeat': 1, 'batch_size': 256, ..., 'num_timesteps': 300000000, ...}
```

  * **內容意思**：這裡列出了本次訓練使用的 PPO 強化學習演算法的所有**超參數 (Hyperparameters)**。
      * `Observation size: 101`：代表神經網路模型每一次的輸入會有 101 個數值。
      * `PPO params`: 包含了學習率 (`learning_rate`)、同時運行的環境數量 (`num_envs: 8192`)、以及您從命令列輸入的總訓練步數 (`num_timesteps: 300000000`) 等重要設定。

-----

#### 4\. 訓練進度回報 (核心部分)

這是判斷訓練好壞最重要的部分，它會週期性地出現：

```
-----------
STEP: 21463040 reward: 219.75445556640625 reward_std: 154.60870361328125
-----------
```

  * **`STEP`**: 代表當前訓練的總步數（timesteps）。這個數字會一直增加，直到達到您設定的 `num_timesteps`。
  * **`reward` (獎勵值)**: **這是評估訓練成效最重要的指標**。它代表在目前的訓練階段，模型在一次完整的模擬（一個 episode）中，平均可以獲得多少分數。**分數越高，代表機器人的表現越好**。
  * **`reward_std` (獎勵標準差)**: 這個值代表了模型表現的**穩定性**。
      * **值越小**，代表每次模擬得到的分數都很接近平均值，模型表現很穩定。
      * **值越大**，代表模型表現時好時壞，不夠穩定。

-----

#### 5\. 模型儲存與匯出

```
Saving checkpoint (step: 21463040): /home/loren/bdx/.../2025_09_09_223111_21463040
 === EXPORT ONNX ===
...
Transferring Dense layer hidden_0, kernel shape (101, 512), bias shape (512,)
...
Weights transferred successfully.
Tensorflow prediction: [ 0.16337563 -0.08779149 ... ]
```

  * **內容意思**：
      * `Saving checkpoint`: 程式將當前的模型權重儲存到您的 `my_checkpoints` 資料夾中。檔名包含了日期和當前的訓練步數，方便您之後選用不同階段的模型。
      * `=== EXPORT ONNX ===`: 開始執行將模型轉換成 ONNX 格式的流程。
      * `Tensorflow prediction`: 這是用匯出後的 ONNX 模型進行一次測試推論的結果，主要是用來確認匯出過程沒有出錯，這個數值本身不直接反映模型的好壞。

### 如何判斷訓練結果的好與壞？

判斷的標準非常直接：**觀察 `reward` 的變化趨勢**。

#### 📈 一個好的訓練過程：

  * **`reward` 持續穩定上升**：隨著 `STEP` 的增加，`reward` 的平均值應該要有一個明顯的上升趨勢。這代表機器人確實從經驗中學到了如何獲得更高的分數。
  * **`reward_std` 相對較小或逐漸變小**：在訓練後期，標準差 `reward_std` 相對於 `reward` 的比例如果能縮小，代表模型學到的策略是穩定且可重複的。

#### 📉 一個壞的訓練過程：

  * **`reward` 停滯不前**：訓練了很久，但 `reward` 一直在一個很低的分數徘徊，沒有上升的跡象。
  * **`reward` 劇烈震盪或崩潰**：`reward` 上升到一定程度後，突然暴跌，並且再也無法回升。這可能是學習率過高或獎勵函數設計不良導致的。

-----

### 分析您兩次的訓練結果

#### 第一次訓練 (22:26 開始)

| STEP | reward (平均獎勵) |
| :--- | :--- |
| 0 | 13.9 |
| 21,463,040 | 219.7 |
| 42,926,080 | 220.4 |
| 64,389,120 | 242.6 |
| 85,852,160 | 267.9 |
| 107,315,200| 275.9 |
| ... | ... |
| **300,482,560**| **252.9** |

  * **分析**：這是一次**非常成功的訓練**！`reward` 從一開始的 13.9，一路飆升到 270 分以上。雖然在訓練末期略有下降，但整體趨勢非常健康，表明模型已經學會了相當不錯的行走策略。

#### 第二次訓練 (05:35 開始)

| STEP | reward (平均獎勵) |
| :--- | :--- |
| 0 | 14.1 |
| 21,463,040 | 219.6 |
| 42,926,080 | 209.0 |
| 64,389,120 | 239.0 |
| 85,852,160 | 272.2 |
| 107,315,200| 269.8 |
| ... | ... |
| **300,482,560**| **248.8** |

  * **分析**：這同樣是一次**成功的訓練**。由於強化學習的隨機性，每次訓練的曲線不盡相同是正常的。這次的 `reward` 也展現了強勁的上升趨勢，最高達到了 303 分，最終穩定在 248 分左右，與第一次的結果相當，證明了您的訓練設定是有效的。

### 總結與下一步

恭喜您！您的兩次訓練都取得了很好的成果。

  * **判斷依據**：主要看 `reward` 是否隨 `STEP` 穩定增長。
  * **視覺化工具**：雖然從文字 LOG 可以判斷，但更專業的方式是使用 **TensorBoard**。您可以在訓練時另外開啟一個終端機，執行 `uv run tensorboard --logdir=my_checkpoints`，就可以在網頁中看到平滑的 `reward` 曲線圖，一目了然。
  * **下一步**：現在您擁有了多個訓練好的模型（儲存在 `my_checkpoints` 資料夾中），可以挑選一個 `reward` 最高的 `.onnx` 檔案，使用 `mujoco_infer.py` 腳本來實際看看機器人學會了什麼樣的走路姿態！


### 要怎麼使用 mujoco_infer.py 腳本來實際看看機器人學會了什麼樣的走路姿態
使用 `mujoco_infer.py` 腳本來觀察您訓練好的模型非常簡單。這個腳本會打開一個 MuJoCo 的互動視窗，載入您的模型，然後您就可以像玩遊戲一樣用鍵盤即時控制機器人。

以下是詳細的步驟：

### 步驟一：挑選您最滿意的模型

首先，您需要從 `my_checkpoints` 資料夾中，挑選一個您認為訓練得最好的模型。判斷的標準就是看訓練 LOG 中哪個模型的 `reward` 分數最高。

根據您提供的 LOG，`STEP: 279019520` 時的 `reward` 達到了 `299.3`，這是一個非常高的分數。對應的 checkpoint 檔案應該是 `2025_09_09_231820_279019520.onnx`。我們就用這個模型來當作範例。

### 步驟二：執行推論 (Inference) 腳本

請打開您的終端機，確保您在 `Open_Duck_Playground` 這個專案的根目錄下，並且已經啟動了 `uv` 的虛擬環境 (如果沒有，請先執行 `source .venv/bin/activate`)。

然後，執行以下指令：

```bash
uv run playground/open_duck_mini_v2/mujoco_infer.py -o my_checkpoints/2025_09_09_231820_279019520.onnx
```

#### 指令解釋：

  * `uv run playground/open_duck_mini_v2/mujoco_infer.py`: 執行推論腳本。
  * `-o my_checkpoints/2025_09_09_231820_279019520.onnx`: 這是**最重要**的參數，`-o` (或 `--onnx_model_path`) 用來指定您要載入的 `.onnx` 模型檔案的路徑。

執行後，您應該會看到一個 MuJoCo 的模擬視窗彈出，裡面有您的 Open Duck Mini V2 機器人。

### 步驟三：在模擬中控制機器人

現在，您可以用鍵盤來給機器人下達移動指令，觀察它學會的走路姿態。

#### 主要控制模式（移動身體）：

| 按鍵 | 功能 |
| :--- | :--- |
| **↑** (上箭頭) | 前進 |
| **↓** (下箭頭) | 後退 |
| **←** (左箭頭) | 向左平移 |
| **→** (右箭頭) | 向右平移 |
| **Q** | 向左旋轉 |
| **E** | 向右旋轉 |
| **H** | **切換到頭部控制模式** |

#### 頭部控制模式：

當您按下 **H** 鍵後，會切換到頭部控制模式，此時的按鍵功能會改變：

| 按鍵 | 功能 |
| :--- | :--- |
| **↑** (上箭頭) | 抬頭（Pitch） |
| **↓** (下箭頭) | 低頭（Pitch） |
| **←** (左箭頭) | 向左轉頭（Yaw） |
| **→** (右箭頭) | 向右轉頭（Yaw） |
| **Q** | 向左歪頭（Roll） |
| **E** | 向右歪頭（Roll） |
| **H** | **切換回主要控制模式** |

### 總結

您只需要簡單的三個步驟：**挑選模型 -\> 執行指令 -\> 鍵盤控制**，就可以親眼看到您花費數小時訓練出來的成果了。試著操作機器人前後左右移動和轉彎，觀察它的步態是否流暢、平衡感如何，這是在強化學習中最有成就感的一刻！


### 要怎麼判讀附件的tensor board圖表
詳細解讀這張圖表的內容，以及如何從中判斷訓練成效。

### 圖表基本元素解析

* **橫軸 (X軸) - Step**:
    * 代表訓練的總步數 (timesteps)，也就是機器人與環境互動的總次數。
    * 這個軸從左到右代表時間的推移和訓練的進展。您可以看到最大值約為 300M，也就是 3 億步，對應您設定的 `--num_timesteps 300000000`。

* **縱軸 (Y軸) - Value**:
    * 代表您所觀察的指標數值。在這張圖中，它代表的是「獎勵值 (Reward)」。

* **曲線 (eval/episode\_reward)**:
    * 這是最重要的曲線，全名是「評估階段的平均單集獎勵 (Evaluation Episode Reward)」。
    * 它顯示了隨著訓練的進行，您的機器人（智慧體）在完成一次完整任務（一個 episode）時，平均能獲得的總分數。
    * **這條曲線是判斷模型好壞最核心的依據。**

* **淡色區域 (平滑後)**:
    * 圖中顏色較淺、半透明的區域代表原始數據的波動範圍或標準差。
    * 顏色較深、實心的線條是經過「平滑 (Smoothing)」處理後的趨勢線。我們在判讀時主要關注這條平滑後的趨勢線。

### 如何判讀這張圖表的訓練結果

從這張圖表來看，這是一次**非常成功的訓練**，原因如下：

1.  **明顯的上升趨勢**：
    * 在訓練初期（大約 0 到 50M 步之間），`eval/episode_reward` 曲線快速且陡峭地向上攀升。這表示機器人處於一個「快速學習期」，它很快就從隨機亂動的狀態，學會了如何站立、移動以獲得基礎分數。

2.  **進入高原期與持續優化**：
    * 在大約 50M 步之後，獎勵值的上升速度開始減緩，進入一個「高原期 (plateau)」。這在強化學習中非常正常，代表模型已經掌握了主要技能。
    * 即便如此，從 50M 步到約 200M 步，獎勵曲線仍然在緩慢但穩定地向上爬升。這表示模型正在進行「微調和優化」，學習更精細、更有效率的動作來獲得更高的分數。

3.  **收斂與穩定**：
    * 在訓練的後期（約 200M 步之後），獎勵曲線趨於平穩，大約在 250-300 分之間波動。這代表模型的性能已經達到了**收斂 (convergence)** 狀態。
    * 在當前的環境設定和獎勵函數下，模型的能力已經達到了極限，再增加訓練時間可能也無法顯著提升分數。這通常意味著訓練可以結束了。

### 總結

* **訓練是好是壞？**
    * **好的**。因為獎勵曲線展現了從快速學習到穩定收斂的完整過程，最終達到了一個相當高的分數。

* **哪個模型最好？**
    * 不一定是分數最高的那個點。通常我們會選擇**進入收斂區域後，表現最穩定的那個模型**。從圖上看，大約在 **200M 到 300M 步之間**產生的模型（checkpoints）都是非常好的候選者。您可以從這個區間挑選幾個模型，實際放到 `mujoco_infer.py` 中觀察它們的走路姿態，來決定哪一個最符合您的期望。

總之，TensorBoard 圖表是一個教科書等級的訓練範例。它清楚地展示了強化學習「從無到有」的學習過程。


### episode_cost 代表甚麼意思?
`episode_cost` (單集成本) 的概念與 `episode_reward` (單集獎勵) 剛好相反，它們共同構成了評估模型好壞的完整畫面。

### `episode_cost` 的核心意義

在強化學習中，我們通常設定一個「獎勵函數」來引導智慧體（Agent，也就是您的機器人模型）學習。這個函數的目標是：

  * **獎勵 (Reward)**：對好的行為給予**正**分數。
  * **懲罰/成本 (Cost/Penalty)**：對不好的行為給予**負**分數。

智慧體的目標是學習一套策略（Policy），使其在一個任務（episode）中獲得的**總分最高**。

**`episode_cost`** 就是將所有**負向的、懲罰性質的分數**加總起來的結果。它代表了機器人在完成一次任務的過程中，付出了多少「代價」。

簡單來說：

  * **`episode_reward`**：您希望看到的**好事**，數值越高越好。
  * **`episode_cost`**：您不希望看到的**壞事**，數值越低越好。

### `episode_cost` 在您的專案中是如何定義的？

在您的程式碼中，這個獎勵與成本的區分是在 `playground/open_duck_mini_v2/joystick.py` 檔案中定義的。

1.  **獎勵與成本的權重設定**：
    在 `joystick.py` 的 `default_config` 函式中，有一個 `reward_config` 區塊：

    ```python
    reward_config=config_dict.create(
        scales=config_dict.create(
            tracking_lin_vel=2.5,      # 正數 -> 獎勵
            tracking_ang_vel=6.0,      # 正數 -> 獎勵
            torques=-1.0e-3,           # 負數 -> 成本
            action_rate=-0.5,          # 負數 -> 成本
            stand_still=-0.2,          # 負數 -> 成本
            alive=20.0,                # 正數 -> 獎勵
            imitation=1.0,             # 正數 -> 獎勵
        ),
        # ...
    ),
    ```

    您可以看到，有些項目的權重（scales）是正數，有些是負數。

2.  **TensorBoard 的日誌邏輯**：
    在 `joystick.py` 的 `step` 函式中，有一段程式碼會根據權重的正負，將指標分別記錄到 TensorBoard 的不同類別下：

    ```python
    for k, v in rewards.items():
        rew_scale = self._config.reward_config.scales[k]
        if rew_scale != 0:
            if rew_scale > 0: # 如果權重是正數
                state.metrics[f"reward/{k}"] = v
            else: # 如果權重是負數
                state.metrics[f"cost/{k}"] = -v
    ```

      * 如果權重是**正數**，它會被歸類為 `reward`。
      * 如果權重是**負數**，它會被歸類為 `cost`。

因此，在您的 `joystick` 任務中，`episode_cost` 主要由以下幾個部分組成：

  * **`cost/torques`**: 懲罰機器人使用過大的馬達力矩（Torque）。這可以鼓勵機器人學習更節能、更柔順的動作。
  * **`cost/action_rate`**: 懲罰機器人做出劇烈、不連續的動作。這可以讓機器人的動作看起來更平滑、穩定。
  * **`cost/stand_still`**: 當指令是站立不動時，懲罰任何多餘的移動。這可以讓機器人在需要時保持穩定。

### 如何判讀 `episode_cost` 圖表？

判讀 `episode_cost` 的方式與 `episode_reward` **完全相反**。

  * **一個好的訓練過程**，您會希望看到 `episode_cost` 的曲線**持續穩定地下降**。這代表您的機器人不僅學會了如何完成任務（獲得 reward），還學會了如何**更有效率、更穩定、更節能**地完成任務。
  * **如果 `episode_cost` 居高不下或持續上升**，即使 `reward` 很高，也可能代表機器人學會了一種「暴力」的、高耗能、不穩定的方式來達成目標。

將 `episode_reward` 和 `episode_cost` 的圖表放在一起看，就能最全面地評估模型的整體表現。理想的狀態是 **`reward` 曲線上升，同時 `cost` 曲線下降**。


### 關於torques的episode_cost，圖表曲線往上升的意義是甚麼?
這張 `cost/torques` 圖表非常有趣，它揭示了您機器人學習策略的一個重要面向。讓我為您詳細解讀。

### 圖表曲線往上升的直接意義

首先，我們回顧一下 `cost` 的概念：它是您**不希望**看到的行為，數值越低越好。

`cost/torques` 這個指標是用來懲罰機器人使用過大的馬達力矩（Torque）。因此，這張圖表的曲線往上升，代表的直接意義是：

**隨著訓練的進行，您的機器人學會了使用越來越大、越來越強的馬達力矩來完成任務。**

換句話說，它正在學習一種更「暴力」、更「積極」的移動策略。

### 為什麼會這樣？這到底是好是壞？

這本身不完全是壞事，但它揭示了強化學習中一個非常核心的概念：**權衡與取捨 (Trade-off)**。

#### 1\. 為什麼機器人要選擇高力矩？

機器人的最終目標是最大化**總獎勵 (`total_reward`)**，而總獎勵是由多個部分組成的：

`總獎勵 = (追蹤速度獎勵 + 存活獎勵 + ...) - (力矩成本 + 動作平滑度成本 + ...)`

在訓練初期（圖表左側），機器人可能還在學習如何保持平衡，動作幅度小，所以力矩成本很低。

隨著訓練推進，機器人發現，如果它敢於做出**更快速、更有力的動作**（這必然會增加力矩），它就能**更精準地追蹤目標速度**（`tracking_lin_vel` 和 `tracking_ang_vel`）。而從獎勵權重來看，追蹤速度帶來的獎勵（權重為 `2.5` 和 `6.0`）遠遠高於力矩帶來的懲罰（權重僅為 `-0.001`）。

因此，機器人「計算」出：**花費一點力矩成本，去換取高額的速度獎勵，是一筆划算的交易**。這就是為什麼 `cost/torques` 曲線會隨著 `eval/episode_reward` 曲線（您上一張圖）一起上升的原因。

#### 2\. 這到底是好是壞？

這取決於您的最終目標：

  * **好的方面**：這表明您的機器人正在學會一種**動態且主動的控制策略**。它不再是小心翼翼地移動，而是變得更加「自信」，能夠利用馬達的力量來快速達成目標。對於需要敏捷反應的任務來說，這是一個好的趨勢。

  * **潛在的壞處**：

      * **能源效率低**：高力矩意味著高耗電量。如果這是一個需要長時間運作的真實機器人，這種策略可能會很快耗盡電池。
      * **硬體損耗**：長期高強度的力矩輸出可能會加速馬達和齒輪箱的磨損。
      * **動作可能不自然**：有時候，過於激進的策略可能會導致機器人動作看起來很僵硬或充滿了急加減速，不夠流暢。

### 結論與下一步建議

**結論**：`cost/torques` 曲線上升，說明您的機器人為了最大化整體獎勵，學會了犧牲能源效率（增加力矩成本）來換取更高的任務表現（追蹤速度獎勵）。考慮到您的總獎勵 `episode_reward` 在持續上升，可以說**在目前的獎勵設定下，這是一個成功的學習策略**。

**下一步建議**：

如果您覺得目前機器人的動作太「暴力」，希望它變得更節能、更柔順，您可以動手調整獎勵權重：

1.  **打開檔案**：`playground/open_duck_mini_v2/joystick.py`
2.  **找到 `reward_config`**。
3.  **增加 `torques` 的懲罰權重**。例如，將 `-1.0e-3` (-0.001) 修改成一個絕對值更大的數字，比如 `-5.0e-3` (-0.005) 或 `-1.0e-2` (-0.01)。
    ```python
    # in joystick.py
    'torques': -5.0e-3,  # 增加懲罰
    ```
4.  **重新訓練**：用新的權重進行訓練，您應該會在 TensorBoard 中看到 `cost/torques` 的上升趨勢變得平緩，甚至可能下降。

這就是強化學習的魅力所在——透過調整獎勵函數，您可以像訓練寵物一樣，塑造您的機器人，讓它學會您期望的行為模式。


### 要如何判讀最佳的最大訓練步數?
這是一個非常專業且核心的問題。在強化學習中，並不存在一個適用於所有情況的「最佳」訓練步數。正確的做法是**透過觀察訓練過程中的各項指標，來判斷模型何時達到「收斂 (Convergence)」狀態**，這時就是最理想的停止點。

您用來判斷的主要工具就是 **TensorBoard**。

### 用來判斷的核心指標

您需要同時觀察以下幾個關鍵圖表，來綜合判斷訓練的狀態：

1.  **`eval/episode_reward` (最重要)**：平均單集獎勵。
2.  **`episode_cost`**：平均單集成本 (例如 `cost/torques` 等的總和)。
3.  **`reward_std`**：獎勵的標準差，代表穩定性。

### 如何判讀並決定最佳停止點

以下是幾種常見的情況，以及它們代表的意義：

#### 情況一：理想的收斂 (Ideal Convergence) - **最佳停止點**

  * **圖表特徵**：

      * `eval/episode_reward` 曲線在經過一段時間的快速上升後，進入一個**平穩的高原期 (plateau)**，不再有明顯的增長。
      * `episode_cost` 曲線也相對應地下降到一個穩定值。
      * `reward_std` 穩定在一個相對較小的值。

  * **意義**：
    這表示模型已經學到了在當前環境和獎勵設定下所能學到的絕大部分知識。繼續訓練下去，它也很難再有突破性的進步。

  * **判斷**：
    當 `reward` 曲線進入高原期並持續一段相當長的時間（例如，幾千萬步）都沒有顯著變化時，就可以認定模型已經收斂。**這個高原期的起點，就是您要找的最佳訓練區間**。再繼續訓練的成本很高，但收益極低。

  * **對應您的圖表**：
    您的 `eval/episode_reward` 圖表就是一個非常典型的理想收斂案例。在大約 **200M (2億) 步**之後，獎勵值就進入了一個穩定的高原期。因此，對您來說，將訓練步數設定在 **200M 到 300M 之間**都是非常合理的。

#### 情況二：邊際效益遞減 (Diminishing Returns)

  * **圖表特徵**：

      * `reward` 曲線還在非常緩慢地上升，但上升的斜率已經變得極度平緩。

  * **意義**：
    模型仍在學習，但學習速度變得非常慢。可能再訓練一億步，`reward` 也只會增加一點點。

  * **判斷**：
    這是一個權衡點。您需要判斷，為了這一點點微小的提升，是否值得花費大量的時間和計算資源。在很多情況下，這個時候也可以考慮停止訓練。

#### 情況三：不穩定或過擬合 (Instability / Overfitting)

  * **圖表特徵**：

      * `reward` 曲線在達到一個峰值後，開始**持續下降**，或者出現劇烈的、大幅度的震盪。

  * **意義**：
    這是一個危險的信號。可能的原因是學習率沒有衰減，導致模型在後期開始「忘記」之前學到的東西，或者過度擬合了某些特定的隨機情況，導致泛化能力下降。

  * **判斷**：
    **應立即停止訓練！** 這種情況下，最佳的模型不是訓練到最後的模型，而是儲存在**獎勵曲線峰值點**的那個 checkpoint。

### 總結與實務流程

1.  **開始一個足夠長的訓練**：像您一樣設定一個較大的 `num_timesteps` (例如 300M 或 500M) 是一個好習慣。
2.  **使用 TensorBoard 監控**：在訓練過程中，持續觀察 `eval/episode_reward` 的曲線。
3.  **尋找高原期**：當您看到獎勵曲線開始變平，進入收斂狀態時，就找到了最佳的訓練區間。
4.  **選擇模型**：從這個高原期區間內儲存的 checkpoints (`.onnx` 檔案) 中，挑選一個 `reward` 數值高且穩定的模型作為您的最終成果。

根據您的圖表，**您將訓練步數設定在 3 億步是完全正確的**，因為它完整地涵蓋了從學習到收斂的整個過程，並讓您能夠從中選出最佳的模型。


### 訓練綜合地形時的觀念

這張圖表顯示「沒有任何規律」和劇烈波動，恰恰說明了您的訓練遇到了一個新的、更困難的挑戰。

這**不是程式錯誤**，而是一個典型的強化學習現象：當任務難度大幅增加時，模型很難找到一個有效的學習路徑。

讓我為您詳細解釋這張圖表的意義，以及為什麼會這樣，最後提供解決這個問題的專業方法。

### 圖表意義詳解

  * **劇烈波動的曲線**：您看到的 `eval/episode_reward` 曲線在高值和低值之間劇烈跳動，不像之前那樣平滑上升。
  * **低迷的平滑趨勢線**：中間那條深色的平滑趨勢線，幾乎是一條沒有上升的水平線，並且一直維持在一個非常低的分數（大約 20-40 分）。

這兩點共同說明了：**您的機器人模型沒有在學習中取得任何進展**。它陷入了一種困境：有時候運氣好，它可能在平地上走得不錯，拿到一些分數；但更多時候，它一遇到崎嶇地面就立刻失敗（摔倒），導致分數極低。最終，平均下來的總獎勵就變得又低又混亂。

### 為什麼在綜合地形上訓練會失敗？

1.  **任務難度指數級增長**：
    讓機器人同時學會「在平地行走」和「在崎嶇地行走」這兩種技能，其難度**遠大於**只學習其中一種。模型需要找到一套能夠適應兩種完全不同環境的通用策略，這對神經網路來說是一個巨大的挑戰。

2.  **「平均」的陷阱**：
    您在 TensorBoard 上看到的是**平均獎勵**。因為您的 `reset` 函式會隨機將機器人放在平地或崎嶇地，所以這個平均值混合了兩種情況：

      * **在平地上的表現 (可能還不錯)**
      * **在崎嶇地上的表現 (極有可能立刻失敗)**
        只要機器人還沒學會如何在崎嶇地面上生存，那麼大量的「零分」表現就會拉低整體的平均分，並造成劇烈的波動。

3.  **探索與懲罰的困境**：
    機器人可能嘗試了幾次在崎嶇地面上行走，但每次都很快摔倒。摔倒會導致它失去未來所有可能的 `alive` 獎勵，這是一個巨大的懲罰。幾次嘗試後，模型可能會學到一個「錯誤的結論」：「崎嶇地面太危險了，最好的策略就是盡量避免在那裡做任何大幅度動作」，這反而阻礙了它去探索和學習正確的策略。

### 如何解決：引入「課程學習 (Curriculum Learning)」

要解決這個問題，最有效、最專業的方法是採用**課程學習**。這個概念就像教小孩子一樣：我們不會讓他直接去考大學，而是先從國小、國中、高中一步步教起。

我們對機器人的訓練也要遵循這個原則。

#### 步驟一：使用「平地專家」模型作為起點 (國小畢業)

您之前在 `flat_terrain` 上已經訓練出了一個非常成功的模型。這個模型已經是「平地行走專家」了。我們要把它的知識傳承下來。

1.  **找到您最好的平地模型**：從 `my_checkpoints` 資料夾中，找到您之前在平地訓練時，`reward` 分數最高的那個模型。例如，我們之前提到的 `2025_09_09_231820_279019520.onnx`。
2.  **找到對應的模型權重資料夾**：與 `.onnx` 檔案對應的，是一個儲存了 JAX 完整權重的資料夾。它的名字應該就是 `2025_09_09_231820_279019520`。

#### 步驟二：在新任務中載入舊權重進行「進階訓練」(上高中)

現在，我們要開始新的 `combined_terrain_backlash` 任務訓練，但不是從零開始，而是站在「平地專家」的肩膀上。

請修改您的訓練指令，加入 `--restore_checkpoint_path` 參數，指向您找到的最佳模型的權重資料夾：

```bash
uv run playground/open_duck_mini_v2/runner.py \
--env joystick \
--task combined_terrain_backlash \
--output_dir my_combined_checkpoints_finetune \
--num_timesteps 1500000000 \
--restore_checkpoint_path my_checkpoints/2025_09_09_231820_279019520 
```

**指令解釋**：

  * `--task combined_terrain_backlash`: 我們仍然使用新的、困難的任務。
  * `--output_dir my_combined_checkpoints_finetune`: 建議使用一個新的資料夾來儲存這次的訓練結果，以便區分。
  * `--restore_checkpoint_path my_checkpoints/2025_09_09_231820_279019520`: **這是最關鍵的一步**。這個參數會讓訓練程式在開始時，先載入您指定的舊模型權重，而不是從隨機權重開始。

### 預期結果

採用課程學習後，您的訓練將會發生質的改變：

  * **更高的起點**：您會看到 TensorBoard 上的 `reward` 曲線一開始就在一個比較高的位置，而不是從零開始。
  * **更穩定的學習**：因為機器人已經掌握了平地行走的基礎，它只需要專注於學習「如何在崎嶇地面上調整步伐以避免摔倒」這個新技能。學習過程會變得更穩定，`reward` 曲線也更有可能呈現出平穩的上升趨勢。

這是解決複雜強化學習任務時非常強大且常用的技巧。請嘗試這個方法，觀察一下訓練曲線的變化。


### 麼從 my_checkpoints 資料夾中找到最好的平地模型
從 `my_checkpoints` 資料夾中找到最好的平地模型，有兩種方法：一種是透過**文字 LOG**，另一種是使用 **TensorBoard** 這個更直觀的工具。

### 方法一：從訓練 LOG 中手動尋找 (您目前可用的方法)

這是最直接的方法。您需要仔細查看您先前貼出的、在平地 (`flat_terrain`) 上訓練的 LOG 檔案。

#### 步驟 1: 理解 checkpoint 的命名規則

首先，要理解程式儲存模型的方式。每一次儲存，程式都會做兩件事：

1.  建立一個資料夾，儲存 JAX 格式的完整權重。
2.  建立一個 `.onnx` 檔案，用於推論。

它們的命名規則是：**`[日期]_[時間]_[當前訓練步數]`**

例如，您 LOG 中的這一行：
`Saving checkpoint (step: 279019520): /.../my_checkpoints/2025_09_09_231820_279019520`

這表示，在訓練進行到 `279,019,520` 步時，程式儲存了名為 `2025_09_09_231820_279019520` 的 JAX 權重資料夾，以及一個同名的 `.onnx` 檔案。

#### 步驟 2: 掃描 LOG，找出最高獎勵 (Reward)

現在，請回顧您貼出的兩份 LOG，我們來找出哪一次訓練的哪個步驟表現最好。

**第一次訓練 (從 `22:26` 開始)**

  * `STEP: 193167360`, `reward: 294.89`
  * `STEP: 279019520`, **`reward: 299.31` (這次訓練的最高點)**
  * `STEP: 300482560`, `reward: 252.90` (分數開始下降)

**第二次訓練 (從 `05:35` 開始)**

  * `STEP: 171704320`, `reward: 281.16`
  * `STEP: 193167360`, **`reward: 303.68` (兩次訓練中的最高點)**
  * `STEP: 214630400`, `reward: 265.84` (分數開始下降)

#### 步驟 3: 鎖定最佳模型檔案

從上面的分析我們可以得出結論：

**在您提供的所有 LOG 中，表現最好的模型是在第二次訓練的第 `193,167,360` 步產生的，當時的平均獎勵達到了 `303.68`。**

根據命名規則，這個最佳模型對應的檔案就是：

  * **JAX 權重資料夾 (用於恢復訓練)**: `my_checkpoints/2025_09_10_061358_193167360`
  * **ONNX 推論檔案 (用於 mujoco\_infer.py)**: `my_checkpoints/2025_09_10_061358_193167360.onnx`

-----

### 方法二：使用 TensorBoard 視覺化尋找 (建議的最佳實踐)

雖然從文字 LOG 中尋找是可行的，但當訓練時間更長、儲存點更多時，這種方法會變得很繁瑣。使用 TensorBoard 是更專業、更高效的方式。

1.  **啟動 TensorBoard**：
    在您的專案根目錄下，開啟一個新的終端機，並執行：

    ```bash
    uv run tensorboard --logdir=my_checkpoints
    ```

      * `logdir=my_checkpoints` 會告訴 TensorBoard 去掃描 `my_checkpoints` 資料夾下的所有訓練紀錄。

2.  **打開網頁**：
    終端機上會顯示一個網址，通常是 `http://localhost:6006/`。將它複製到您的瀏覽器中打開。

3.  **尋找峰值**：
    在 TensorBoard 的介面中，找到 `eval/episode_reward` 這張圖表。您可以用滑鼠在曲線上移動，圖表會顯示出每個數據點的具體 `Step` 和 `Value` (獎勵值)。您只需要找到曲線的**最高點 (峰值)**，記下那個點的 `Step` 數。

    *(這是一張示意圖，展示如何在曲線上找到峰值)*

4.  **對應檔案**：
    根據您在 TensorBoard 上找到的最佳 `Step` 數，回到 `my_checkpoints` 資料夾中，找到對應的 checkpoint 檔案即可。

**總結：** 透過分析您提供的 LOG，**您應該使用的最佳平地模型是 `my_checkpoints/2025_09_10_061358_193167360`**。


### 要如何看見機器人在combined_terrain_backlash內的實際狀況
好的，要觀看您在 `combined_terrain_backlash` 這個新場景中訓練出來的機器人，方法和之前類似，但需要一個**關鍵的額外步驟**，因為您需要明確告訴推論腳本（`mujoco_infer.py`）去載入這個新的、更複雜的場景檔案。

### 步驟一：挑選在「綜合地形」上訓練出的最佳模型

首先，您需要從這次「課程學習」的訓練成果中，挑選一個表現最好的模型。

1.  **打開 TensorBoard**：
    執行 `uv run tensorboard --logdir=my_combined_checkpoints_finetune` 來查看這次訓練的曲線。

2.  **尋找最佳模型**：
    觀察 `eval/episode_reward` 曲線，找到進入收斂高原期後，分數最高的那個點。根據您提供的圖表，看起來最好的模型大約出現在 **90M (九千萬) 步之後**。您可以從這個區間選擇一個 `.onnx` 檔案。

### 步驟二：執行推論腳本並指定新場景

這是最關鍵的一步。在執行 `mujoco_infer.py` 時，我們除了要用 `-o` 指定模型，還需要用 `--model_path` 參數來**指定要載入的場景 XML 檔案**。

假設您挑選了在 `my_combined_checkpoints_finetune` 資料夾中，約 90M 步時儲存的模型，請執行以下指令：

```bash
uv run playground/open_duck_mini_v2/mujoco_infer.py \
--onnx_model_path my_combined_checkpoints_finetune/[您的模型檔案名稱].onnx \
--model_path playground/open_duck_mini_v2/xmls/scene_combined_terrain_backlash.xml
```

**指令解釋**：

  * `--onnx_model_path`: 指定您從**綜合地形訓練**中選出的最佳 `.onnx` 模型。請記得替換 `[您的模型檔案名稱]`。
  * `--model_path`: **(這一步是新的！)** 我們明確地告訴腳本，不要載入預設的平地場景，而是載入我們新建的 `scene_combined_terrain_backlash.xml` 綜合場景。

### 步驟三：在模擬中觀察

執行指令後，您會看到 MuJoCo 視窗彈出。這次，您會看到一個非常寬闊的場景，一半是平坦的棋盤格，另一半（在遠處）是崎嶇的地形。

**重要提示：**

因為您在 `joystick.py` 中設定了機器人會**隨機**出生在平地（X=0 附近）或崎嶇地（X=20 附近），所以：

  * 您可能需要**重新啟動幾次** `mujoco_infer.py` 腳本，才能看到機器人剛好出生在崎嶇地形上。
  * 您也可以嘗試在模擬中，用鍵盤控制機器人**從平地區域走向崎嶇區域**，直接觀察它在穿越地形邊界時的反應和適應能力。這也是測試模型泛化能力的一個好方法！


### 為什麼每次訓練出來的.onnx檔案大小都是864KB，檔案越大是不是越好?
這是一個非常好的問題，它觸及了神經網路模型的核心概念！

我將從兩個方面來回答您的問題：

### 1\. 為什麼每次訓練出來的 .onnx 檔案大小都是 864KB？

**簡單來說：因為您的神經網路「結構」是固定的。**

  * **.onnx 檔案儲存了什麼？**

    1.  **網路結構 (Architecture)：** 包含了神經網路有多少層，每層有多少個神經元，以及它們之間如何連接。這就像是一棟建築物的藍圖，決定了建築物的規模和房間數量。
    2.  **網路權重 (Weights)：** 在訓練過程中，AI 學習到的所有參數數值。這就像是建築物蓋好後，每個房間裡的具體陳設和裝潢。

  * **結構在哪裡定義？**
    在您的專案中，這個「藍圖」是在 `playground/common/runner.py` 檔案裡的 `make_ppo_networks` 函式中定義的。其中，最關鍵的一行是：

    ```python
    hidden_layer_sizes: tuple[int, ...] = (256, 128, 64),
    ```

    這行程式碼定義了您的神經網路有三個隱藏層，神經元數量分別是 256、128 和 64。因為這個結構從未改變，所以網路中需要儲存的「權重」總數量也是一個固定的常數。

  * **訓練做了什麼？**
    訓練的過程，並不是在改變網路的「結構」，而是在不斷地調整、優化那些固定數量的「權重」數值，以找到一組能讓機器人表現最好的權重組合。

**結論：** 因為您每次訓練的都是同一個結構（同樣大小的藍圖），所以無論最終學到的權重（房間裝潢）是什麼，儲存整個模型所需的空間（檔案大小）都是完全一樣的，也就是 864KB。

-----

### 2\. 檔案越大是不是越好？

**答案是：不，檔案越大不一定越好。** 這是在 AI 領域一個非常經典的「權衡 (Trade-off)」。

一個更大的檔案，意味著一個更複雜、擁有更多神經元和層數的網路。

#### 大型模型 (檔案較大)

  * **優點：**
      * **學習能力上限高：** 像一個擁有更多腦細胞的大腦，有潛力學習更複雜、更精細的動作和策略。
  * **缺點：**
      * **容易「過擬合 (Overfitting)」：** 這是最大的風險。模型可能會「死記硬背」訓練中的特定情境，而不是學會通用的解決方案。這會導致模型在訓練環境中表現很好，但一換到稍微不同的新環境（例如，從平地換到微小的斜坡）就表現得很差。
      * **訓練和推論速度慢：** 需要計算的參數更多，會導致訓練時間更長，並且最終部署到機器人上時，每次決策所需的時間也更長，可能影響即時反應。
      * **需要更多訓練資料：** 複雜的模型需要海量的資料才能學好，否則更容易過擬合。

#### 小型模型 (檔案較小，像您目前的 864KB)

  * **優點：**
      * **訓練和推論速度快：** 更少的計算量，效率更高。
      * **泛化能力通常更好：** 不容易過擬合，傾向於學習更通用的規律，對新環境的適應性可能更強。
      * **資料需求量較小。**
  * **缺點：**
      * **學習能力有上限：** 對於極其複雜的問題，可能會因為模型不夠大而無法學到最佳解。

### 總結與建議

對於您目前的機器人控制任務，`(256, 128, 64)` 這個結構以及它所產生的 864KB 模型，是一個非常合理且優秀的起點。

**我的建議是：** 在您發現目前的模型尺寸確實無法學習您想要的複雜行為之前，**請優先專注於調整我們之前討論過的「獎勵函數」和「目標姿態」**。優化訓練方法，遠比盲目地增大模型尺寸更為重要且有效。一個好的獎勵函數，可以讓一個小模型發揮出驚人的效果。


### 怎麼使用 使用 ref_motion_viewer.py 來視覺化 polynomial_coefficients.pkl 產生的參考動作
好的，這份 `ref_motion_viewer.py` 程式碼可以讓您在 MuJoCo 模擬環境中，載入並視覺化 `polynomial_coefficients.pkl` 檔案所定義的參考動作（Reference Motion）。

以下是使用這個腳本的步驟和說明：

### 1\. 準備工作

請確認您已經安裝好所有必要的 Python 套件，特別是 `mujoco` 和 `pygame`（如果您想用搖桿控制）。

### 2\. 執行腳本

您需要在您的專案根目錄（`Open_Duck_Playground/`）下打開終端機 (Terminal)，然後執行以下指令：

```bash
uv run playground/open_duck_mini_v2/ref_motion_viewer.py --reference_data playground/open_duck_mini_v2/data/polynomial_coefficients.pkl
```

**指令詳解：**

  * `uv run playground/open_duck_mini_v2/ref_motion_viewer.py`: 這是執行的 Python 腳本檔案路徑。
  * `--reference_data playground/open_duck_mini_v2/data/polynomial_coefficients.pkl`: 這是最重要的參數。它告訴腳本要去哪裡讀取參考動作的資料。您必須將路徑指向 `open_duck_mini_v2` 資料夾內的 `polynomial_coefficients.pkl` 檔案。

執行後，您會看到一個 MuJoCo 的模擬視窗彈出，裡面有您的 Open Duck Mini V2 機器人。

### 3\. 如何在模擬器中互動

腳本啟動後，機器人會先呈現預設的站立姿勢。您可以透過鍵盤或搖桿來下指令，讓機器人做出不同的動作。

#### **鍵盤控制**\*\*:\*\*

  * **向上箭頭 (↑)**: 向前移動 (增加 `dx` 速度)。
  * **向下箭頭 (↓)**: 向後移動 (減少 `dx` 速度)。
  * **向左箭頭 (←)**: 向左平移 (增加 `dy` 速度)。
  * **向右箭頭 (→)**: 向右平移 (減少 `dy` 速度)。
  * **'Q' 鍵**: 向左旋轉 (增加 `dtheta` 角速度)。
  * **'E' 鍵**: 向右旋轉 (減少 `dtheta` 角速度)。

當您按下這些按鍵時，終端機中會印出目前的指令數值，例如 `command: [0.15, 0.0, 0.0]`，同時模擬器中的機器人會開始執行對應的動作。

#### **搖桿控制 (Joystick)\*\*\*\*:**

如果您有連接搖桿，可以在執行指令時加上 `--joystick` 參數：

```bash
uv run playground/open_duck_mini_v2/ref_motion_viewer.py --reference_data playground/open_duck_mini_v2/data/polynomial_coefficients.pkl --joystick
```

  * **左搖桿的上下**: 控制前後移動。
  * **左搖桿的左右**: 控制左右平移。
  * **右搖桿的左右 (如果偵測到第二支搖桿)**: 控制旋轉。

### 4\. 觀察動作對稱性

透過這個視覺化工具，您可以直接觀察機器人**在靜止站立 (`command` 為 `[0, 0, 0]`)** 以及**在不同方向移動時**，左右腿的動作是否對稱。

如果 `polynomial_coefficients.pkl` 內的參考動作資料本身就不對稱，您會在這個模擬器中清楚地看到機器人出現歪斜或不協調的步態。這將能幫助您判斷問題是出在參考動作資料，還是如我們先前所討論的，出在獎勵函數的設計上。





好的，這沒有問題。我已經詳細分析了您提供的 `mujoco_infer.py` 檔案中的 `key_callback` 函式，為您整理出模擬器中所有按鍵的功能配置。

模擬器的控制主要分為三種模式：**身體控制模式**、**頭部控制模式**和**手動姿態調整模式**。

---

### **通用模式切換**

| 按鍵 | 功能 | 說明 |
| :--- | :--- | :--- |
| **H** | 切換「身體控制」/「頭部控制」模式 | 這是最主要的切換鍵。按下後會在兩種模式之間循環切換。 |
| **M** | 切換「AI 控制」/「手動姿態調整」模式 | 按下後會暫停 AI 的所有動作，讓您可以進入 MuJoCo 的 UI 介面手動拖動關節。再次按下則恢復 AI 控制。 |

---

### **1. 身體控制模式 (預設模式)**

這個模式用來控制機器人的移動。當 `head_control_mode` 為 `False` 時啟用。

| 按鍵 | 功能 |
| :--- | :--- |
| **W** | 前進 |
| **S** | 後退 |
| **A** | 向左平移 |
| **D** | 向右平移 |
| **Q** | 向左旋轉 |
| **E** | 向右旋轉 |

---

### **2. 頭部控制模式**

這個模式用來控制機器人的頭部姿態。當 `head_control_mode` 為 `True` 時啟用。

| 按鍵 | 功能 | 控制的關節 |
| :--- | :--- | :--- |
| **I** (方向鍵上) | 頭部向上看 | `neck_pitch`, `head_pitch` |
| **K** (方向鍵下) | 頭部向下看 | `neck_pitch`, `head_pitch` |
| **J** (方向鍵左) | 頭部向左轉 | `head_yaw` |
| **L** (方向鍵右) | 頭部向右轉 | `head_yaw` |
| **U** | 頭部向左側傾 (Roll) | `head_roll` |
| **O** | 頭部向右側傾 (Roll) | `head_roll` |

---

### **3. 步態頻率調整 (僅在身體控制模式下)**

這組按鍵用來微調機器人走路時的步態節奏，但它只在**非站立模式** (`standing` 為 `False`) 下才有效。

| 按鍵 | 功能 | 說明 |
| :--- | :--- | :--- |
| **P** | 增加步態頻率 | 讓機器人走路的步伐變快。 |
| **;** (分號) | 減少步態頻率 | 讓機器人走路的步伐變慢。 |

希望這份完整的按鍵配置表能幫助您更順利地操作模擬器！


### 長時間訓練後記憶體耗盡，錯誤訊息 Allocator (GPU_0_bfc) ran out of memory trying to allocate 512.0KiB

你好，這個錯誤日誌非常清楚，問題的根源不在我們修改的獎勵邏輯，而是一個更底層的硬體資源問題：**您的 GPU 記憶體 (VRAM) 在長時間訓練後耗盡了**。

這是一個在進行大規模 AI 訓練時很常見的問題，特別是在長時間運行後。讓我為您詳細解釋一下發生的原因以及解決方法。

### 錯誤分析：為何會記憶體耗盡？ 🧠

從您的日誌中可以看到，訓練本身是順利開始的。錯誤發生在訓練進行到 `step: 10813440` 時，程式嘗試儲存檢查點並**匯出 ONNX 模型**的環節。

1.  **主要訓練程序**：您的訓練 (`runner.py`) 主要使用 JAX 函式庫，它會佔用一部分 GPU 記憶體來儲存模型參數和計算梯度。
2.  **ONNX 匯出程序**：當需要匯出模型時，程式會呼叫 `export_onnx.py`。這個檔案使用了 **TensorFlow/Keras** 函式庫來建立一個網路結構並載入 JAX 的權重，然後再將其轉換為 ONNX 格式。
3.  **資源衝突**：問題就在於，TensorFlow 在啟動時，會試圖向 GPU 申請一大塊記憶體。在訓練剛開始時，記憶體還很充裕；但經過長時間訓練後，JAX 使用的記憶體可能產生了碎片，或者總量已經很高，導致 TensorFlow 無法申請到它需要的記憶體，最終引發了 `RESOURCE_EXHAUSTED` 錯誤。

錯誤訊息 `Allocator (GPU_0_bfc) ran out of memory trying to allocate 512.0KiB` 就明確指出了這一點。

-----

### 解決方案

要解決這個問題，我們需要從「減少記憶體使用」或「優化記憶體管理」的角度著手。以下提供建議的解決方案，您可以從第一種開始嘗試。

#### **方案一：嘗試 TensorFlow 的實驗性記憶體配置器**

錯誤日誌本身提供了一個建議：`maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation`。這個設定可以讓 TensorFlow 的記憶體管理更有彈性，有時可以避免因碎片化導致的問題。

您可以在執行訓練指令前，先在終端機中設定這個環境變數：

```bash
export TF_GPU_ALLOCATOR=cuda_malloc_async
uv run playground/open_duck_mini_v2/runner.py --env joystick ... (您的其他參數)
```

-----

#### **方案二：降低訓練的記憶體佔用**

如果以上方法都無效，最後的手段就是降低訓練本身佔用的記憶體，為 ONNX 匯出預留更多空間。

您可以在執行指令時，透過參數來減少同時運行的環境數量。

  * **PPO 參數日誌**：`'num_envs': 8192`

  * **修改後的執行指令**：

    ```bash
    # 將 num_envs 從 8192 降低到 4096 或 2048
    uv run playground/open_duck_mini_v2/runner.py --num_envs 4096 ... (您的其他參數)
    ```

**優缺點**：這個方法一定能減少記憶體使用，但同時也會降低訓練的效率，讓訓練達到同樣效果所需的時間變長。

建議您從**方案一**開始嘗試，因為它在不影響訓練效率的情況下，最有可能直接解決這個問題。


### 修改初始動作（Initial Pose）是調整機器人行為的基礎
好的，這個問題問得很好。修改初始動作（Initial Pose）是調整機器人行為的基礎步驟之一。

您可以在定義機器人模型的 **XML 檔案**中修改初始的各關節角度。

在您的專案中，主要有兩個版本的模型檔案，一個有背隙（backlash），一個沒有。您需要修改的是您在訓練時使用的那一個版本。

  * **無背隙版本**: `playground/open_duck_mini_v2/xmls/open_duck_mini_v2.xml`
  * **有背隙版本**: `playground/open_duck_mini_v2/xmls/open_duck_mini_v2_backlash.xml`

### **修改流程**

1.  **找到關鍵程式碼**:
    在訓練環境檔案 `playground/open_duck_mini_v2/joystick.py` 中，程式碼透過讀取一個名為 `"home"` 的**關鍵影格 (keyframe)** 來設定初始姿態：

    ```python
    self._init_q = jp.array(self._mj_model.keyframe("home").qpos)
    ```

2.  **定位並修改 XML 檔案**:
    因此，我們需要去 XML 檔案中找到這個名為 `"home"` 的關鍵影格，並修改它的 `qpos` (position) 屬性。

    打開 `open_duck_mini_v2_backlash.xml` (以背隙版本為例)，您會在檔案底部附近找到這樣一行：

    ```xml
    <key name="home" qpos="0 0 0.28 1 0 0 0 0 0 0 0 -0.5 0 1 0 -0.5 0 0 0 0 0 -0.5 0 1 0 -0.5 0" ctrl="0 0 -0.523 1.047 -0.523 0 0 0 0 0 0 -0.523 1.047 -0.523"/>
    ```

3.  **理解 `qpos` 的數值**:
    您需要修改的就是 `qpos="..."` 引號中的那一長串數字。這些數字代表了機器人所有關節的初始位置，順序如下：

      * **前 7 個數字 (浮動基座)**:

          * `0 0 0.28`: 身體軀幹的初始 `(x, y, z)` 座標。
          * `1 0 0 0`: 身體軀幹的初始姿態，以四元數 `(w, x, y, z)` 表示。`1 0 0 0` 代表沒有任何旋轉。

      * **後續數字 (馬達關節)**:

          * 這些數字對應模型中定義的各個關節的初始角度（單位是弧度），並且因為背隙的存在，每個馬達關節後面都跟著一個背隙關節的初始值 (通常是0)。
          * 例如，序列中的 `-0.5` 和 `1` 分別對應了 `hip_pitch` 和 `knee` 的初始彎曲角度，這也是造成預設站姿是 V 型的原因。

### **如何修改**

如果您希望機器人以一個**完全筆直的站姿**開始，您可以嘗試將所有對應馬達關節的 `qpos` 數值都修改為 `0`，只保留 `z` 軸的高度。

**修改範例 (筆直站姿)**：

```xml
<key name="home" qpos="0 0 0.28 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0" ctrl="..."/>
```

「背隙」(Backlash) 是機械工程和機器人學中的一個重要概念，理解它有助於您更精準地調整和訓練模型。

簡單來說，「背隙關節數值」代表的是一個**模擬出來的、微小的、非驅動的關節**的當前角度。這個關節被用來模仿真實世界中，齒輪或連桿之間因為製造公差而存在的微小「間隙」或「鬆動」。

讓我為您詳細解釋：

### 1\. 什麼是「背隙」？(一個生活中的例子)

您可以想像一下舊式的門把手。當您要開門時，您轉動門把手，可能會感覺到門把手**空轉了一小段角度**後，才真正帶動門鎖的機械結構。這個「空轉」的行程，就是「背隙」。

在機器人的齒輪箱中，當馬達從一個方向轉動，然後反轉到另一個方向時，也會因為齒輪之間的微小間隙，產生類似的短暫延遲或「空檔」。

### 2\. MuJoCo 如何模擬背隙？

為了讓模擬更貼近真實，MuJoCo 物理引擎採用了一種巧妙的方法：它在您主要的、由馬達驅動的關節（例如 `left_hip_pitch`）後面，**串聯**上另一個**沒有馬達**的、只能在一個**極小範圍**內自由活動的關-節。這個額外的小關節，就是「背隙關節」。

  * **馬達關節 (`left_hip_pitch`)**：由 AI 的 `action` 指令驅動，負責主要的運動。
  * **背隙關節 (`left_hip_pitch_backlash`)**：沒有馬達，它只是被動地跟隨著馬達關節運動。但因為它有一個小小的活動範圍（例如 -1° 到 +1°），所以當馬達關節反轉方向時，它會在帶動最終的連桿之前，先在這個小範圍內「晃動」一下。

### 3\. 「背隙關節數值」在 XML 檔案中代表什麼？

現在我們來看您專案中的 `open_duck_mini_v2_backlash.xml` 檔案。

當您看到 `<key name="home" qpos="..." .../>` 這一行時，`qpos` 裡面的數值代表了**所有關節**的初始角度。因為背隙關節的存在，這個序列會是這樣排列的：

`... [馬達關節A的角度], [背隙關節A的角度], [馬達關節B的角度], [背隙關節B的角度] ...`

**舉例來說，在您檔案的 `home` 關鍵影格中**：

```xml
<key name="home" qpos="... 0 -0.5 0 1 0 -0.5 0 ..." ... />
```

假設這一段對應的是左腿：

  * **`0`**: `left_hip_roll` 的初始角度。
  * **`-0.5`**: `left_hip_pitch` 的初始角度。
  * **`0`**: `left_hip_pitch_backlash` **的初始角度**。
  * **`1`**: `left_knee` 的初始角度。
  * **`0`**: `left_knee_backlash` **的初始角度**。
  * ... 以此類推。

**「背隙關節數值」(例如這裡的 `0`)** 的意義是：在模擬開始的那一刻，這個代表「間隙」的微小關節，其角度**正好處於其活動範圍的正中間**。

在模擬過程中，當馬達來回轉動時，這個數值會在它被允許的微小範圍內（例如 -0.017 到 +0.017 弧度）不斷變化，從而產生了模擬真實世界「鬆動」或「延遲」的效果。

**總結**：
「背隙關節數值」就是用來模擬機械間隙的那個**虛擬小關節**的當前角度。在設定初始姿態時，它通常被設為 0，代表一個沒有偏向任何一邊的「置中」狀態。

### qpos 各項數值說明
這串數字是機器人模型中，除了身體浮動基座之外，所有**馬達關節**與**背隙關節**的初始角度設定 (`qpos`)。

這些數字是成對出現的，每一對分別代表：`[馬達關節角度] [背隙關節角度]`。

以下是根據您專案的模型結構 (`open_duck_mini_v2_backlash.xml`)，對這串數字的逐行詳細解讀 (角度單位為**弧度**)：

---

### **第一部分：左腿 (Left Leg)**

| 數值 | 對應關節 | 說明 |
| :--- | :--- | :--- |
| `0.002` | `left_hip_yaw` | 左髖關節的平轉 (Yaw) |
| `0` | `left_hip_yaw_backlash` | 對應的背隙關節 |
| `0.053` | `left_hip_roll` | 左髖關節的側傾 (Roll) |
| `0` | `left_hip_roll_backlash` | 對應的背隙關節 |
| `-0.63` | `left_hip_pitch` | 左髖關節的前後擺動 (Pitch)，負值代表向後 |
| `0` | `left_hip_pitch_backlash`| 對應的背隙關節 |
| `1.368` | `left_knee` | 左膝關節彎曲 |
| `0` | `left_knee_backlash` | 對應的背隙關節 |
| `-0.784`| `left_ankle` | 左腳踝關節彎曲 |
| `0` | `left_ankle_backlash` | 對應的背隙關節 |

---

### **第二部分：頭部 (Head)**

| 數值 | 對應關節 | 說明 |
| :--- | :--- | :--- |
| `0` | `neck_pitch` | 頸部的前後擺動 (Pitch) |
| `0` | `head_pitch` | 頭部的前後擺動 (Pitch) |
| `0` | `head_yaw` | 頭部的平轉 (Yaw) |
| `0` | `head_roll` | 頭部的側傾 (Roll) |

*(註：頭部關節在這個模型中沒有設定背隙關節)*

---

### **第三部分：右腿 (Right Leg)**

| 數值 | 對應關節 | 說明 |
| :--- | :--- | :--- |
| `-0.003`| `right_hip_yaw` | 右髖關節的平轉 (Yaw) |
| `0` | `right_hip_yaw_backlash` | 對應的背隙關節 |
| `-0.065`| `right_hip_roll` | 右髖關節的側傾 (Roll) |
| `0` | `right_hip_roll_backlash`| 對應的背隙關節 |
| `0.635` | `right_hip_pitch` | 右髖關節的前後擺動 (Pitch)，正值代表向前 |
| `0` | `right_hip_pitch_backlash`| 對應的背隙關節 |
| `1.379` | `right_knee` | 右膝關節彎曲 |
| `0` | `right_knee_backlash` | 對應的背隙關節 |
| `-0.796`| `right_ankle` | 右腳踝關節彎曲 |
| `0` | `right_ankle_backlash` | 對應的背隙關節 |

**總結**：
這整串數字精確地定義了機器人在模擬開始時的初始姿態。第一列是主要馬達關節的角度，第二列則是模擬機械間隙的背隙關節的角度（在初始狀態下通常為 0）。

### 為什麼 right_hip_roll 跟 left_hip_roll 的初始數值會有差異
您提出了一個觀察極其敏銳的問題！在一個理想的、完全對稱的模型中，`right_hip_roll` 和 `left_hip_roll` 的初始值確實應該是大小相等、符號相反的（例如 `0.05` 和 `-0.05`）。

您提供的數值 (`left_hip_roll: 0.053` 和 `right_hip_roll: -0.065`) 之所以會有這種微小的差異，並不是隨機的錯誤，而往往是出於**刻意**的、基於**物理現實**的工程調校。

主要有以下幾個可能的原因：

### 1. **不對稱的重心 (Center of Mass)**

這是最常見的原因。雖然機器人在 3D 模型上看是對稱的，但在真實的硬體上，內部的電子元件（如電池、主機板、感測器）的重量分佈幾乎不可能是完美對稱的。

* **問題**：假設機器人內部的電池稍微偏向左側，那麼即使雙腿姿態完全對稱，機器人的重心也會偏左，導致它有向左傾倒的趨勢。
* **解決方案**：為了在靜止時就抵銷掉這個不平衡的力矩，工程師會有意地在初始姿態中加入一個微小的不對稱。讓右腿的 `hip_roll` (`-0.065`) 比左腿 (`0.053`) 更向外「撐開」一點點，就可以產生一個向右的支撐力，剛好抵銷掉偏左的重心，讓機器人站得更穩。

### 2. **感測器校準誤差**

機器人賴以感知自身姿態的慣性測量單元（IMU）可能存在微小的安裝誤差或校準偏差。

* **問題**：例如，IMU 可能被安裝得稍微歪斜了 0.1 度。這會導致即使機器人被放置在絕對水平的地面上，IMU 仍然會向主控回報「我正在向右傾斜 0.1 度」。
* **解決方案**：為了讓機器人在「自我感覺」上是直立的，初始姿態可能會被設定為向左傾斜 0.1 度來進行「補償」。這種補償反映在 `hip_roll` 的初始值上，就會造成不對稱。

### 3. 「模擬到現實 (Sim-to-Real)」的遷移調整

這些數值很可能是從真實的物理機器人上「反向工程」回來的。

* **過程**：工程師在真實的機器人上進行手動調校，他們會微調各個關節的角度，直到找到一個能讓機器人站得最穩、最久的姿態。由於真實世界的各種不完美（零件公差、組裝誤差、線材拉扯等），這個最佳姿態幾乎不可能是完美對稱的。
* **結果**：一旦在現實中找到了這組「黃金數值」，工程師就會將它們複製回模擬環境的 XML 檔案中，作為初始姿態。這樣可以讓模擬的起點更接近物理現實，有助於訓練出能夠更好地應用在真實機器人上的 AI 模型。

**總結**：
您看到的 `right_hip_roll` 和 `left_hip_roll` 的數值差異，很可能不是一個 Bug，而是一個**為了應對物理世界的不完美，所做出的高精度工程妥協**。它是讓模擬更貼近現實、讓機器人更穩定的關鍵細節。


### 怎麼調整機器人在訓練環境內模擬的重量
好的，調整機器人在訓練環境內的模擬重量是一個核心的物理參數修改，這需要在定義機器人模型的 **XML 檔案**中進行。

修改機器人的重量會直接影響它的慣性、平衡和動力學特性，因此這是一個非常重要的調整。

### **修改流程**

1.  **定位核心模型檔案**：
    首先，您需要找到定義機器人身體部件物理屬性的主要 XML 檔案。在您的專案中，這個檔案是 `playground/open_duck_mini_v2/xmls/open_duck_mini_v2.xml`。
    *(註：`..._backlash.xml` 檔案通常會包含這個核心檔案，所以修改核心檔案是最好的做法)*

2.  **找到身體部件 (Body) 的定義**：
    打開 `open_duck_mini_v2.xml` 檔案。您需要找到定義機器人主要軀幹的 `<body ...>` 標籤。根據 `constants.py` 檔案的定義，機器人的根軀幹名為 `"trunk_assembly"`。

    您會在 XML 檔案中找到類似下面這段的程式碼：

    ```xml
    <body name="trunk_assembly" pos="0 0 0" quat="1 0 0 0">
        <inertial pos="0.006 0.001 -0.012" quat="0.999 0 0.004 0.016" mass="0.408" diaginertia="0.001 0.001 0"/>
        <geom type="mesh" contype="1" conaffinity="1" group="1" material="trunk_plastic" mesh="trunk_top"/>
        ...
        ...
    </body>
    ```

3.  **修改 `mass` 屬性**：
    **要調整的關鍵數值就在 `<inertial ... />` 這一行裡面。**

      * `mass="0.408"`：這個屬性定義了 `"trunk_assembly"` 這個身體部件的**質量（重量）**，單位是公斤 (kg)。
      * 要增加機器人的重量，您可以將 `0.408` 這個數值**調高**（例如 `mass="0.6"`）。
      * 要減輕機器人的重量，您可以將這個數值**調低**（例如 `mass="0.3"`）。

### **重要注意事項**

  * **修改所有相關部件**：一個完整的機器人是由多個身體部件 (`<body>`) 組成的（例如軀幹、大腿、小腿等）。如果您想整體性地增加或減少機器人的重量，您需要按照比例去修改**每一個**部件的 `mass` 屬性，而不僅僅是軀幹。
  * **需要重新訓練**：**這是一個非常重要的步驟**。修改了機器人的重量後，它的物理行為會發生巨大改變。舊的模型是基於舊的重量學習的，將無法適應新的物理環境。因此，在修改 XML 檔案後，您必須**刪除舊的檢查點 (`my_checkpoints` 資料夾)，然後重新開始一個全新的訓練**，讓 AI 從頭開始學習如何控制這個新的、更重（或更輕）的身體。

**總結**：
通過修改模型 XML 檔案中，各個 `<body>` 標籤下的 `<inertial mass="..." />` 屬性，您就可以精確地調整機器人在模擬環境中的重量。


#### 請問mujoco_infer.py內的mujoco模擬器所有的鍵盤按鍵及對應的指令
好的，這沒有問題。為了給您一份最完整的列表，我將結合兩方面的資訊：

1.  **程式自定義按鍵**：來自您專案中 `mujoco_infer.py` 檔案 `key_callback` 函式的設定。
2.  **MuJoCo 模擬器預設按鍵**：所有 MuJoCo 模擬器視窗通用的內建快速鍵，用於控制視角、模擬、渲染等。

以下是這兩部分整合後的完整鍵盤指令列表：

---

### **一、 機器人專用控制指令**

這些是 `mujoco_infer.py` 程式中特別為機器人定義的控制按鍵。

#### **通用模式切換**

| 按鍵 | 功能 | 說明 |
| :--- | :--- | :--- |
| **H** | 切換「身體控制」/「頭部控制」模式 | 在移動機器人和控制頭部兩種模式間切換。 |
| **M** | 切換「AI 控制」/「手動姿態調整」模式 | 暫停 AI，讓您可以在 UI 中手動拖動關節。 |

#### **1. 身體控制模式 (預設)**

| 按鍵 | 功能 |
| :--- | :--- |
| **↑** (方向鍵上) | 前進 |
| **↓** (方向鍵下) | 後退 |
| **←** (方向鍵左) | 向左平移 |
| **→** (方向鍵右) | 向右平移 |
| **Q** | 向左旋轉 |
| **E** | 向右旋轉 |
| **P** | 增加步態頻率 |
| **;** (分號) | 減少步態頻率 (程式碼中註記為 m，但 keycode 59 實際是分號)|

#### **2. 頭部控制模式 (按 H 鍵後)**

| 按鍵 | 功能 |
| :--- | :--- |
| **↑** (方向鍵上) | 頭部向上看 |
| **↓** (方向鍵下) | 頭部向下看 |
| **←** (方向鍵左) | 頭部向左轉 |
| **→** (方向鍵右) | 頭部向右轉 |
| **Q** | 頭部向左側傾 (Roll) |
| **E** | 頭部向右側傾 (Roll) |

---

### **二、 MuJoCo 模擬器通用快速鍵**

這些是 MuJoCo Viewer 內建的預設按鍵，在任何模擬中都通用。

#### **模擬控制**

| 按鍵 | 功能 |
| :--- | :--- |
| **Space** | 播放 / 暫停模擬 |
| **Backspace** | 重置模擬 |
| **→** (右方向鍵) | (暫停時) 單步前進 |
| **←** (左方向鍵) | (暫停時) 單步後退 |

#### **攝影機/視角控制**

| 操作 | 功能 |
| :--- | :--- |
| **滑鼠右鍵 + 拖曳** | 旋轉視角 (Orbit) |
| **滑鼠中鍵 + 拖曳** | 平移視角 (Pan) |
| **滑鼠滾輪** | 縮放視角 (Zoom) |
| **Ctrl + 滑鼠右鍵 + 拖曳** | 旋轉視角 (另一種模式) |
| **A** | 自動調整攝影機 |
| **F** | 切換跟隨的攝影機 |
| **[** and **]** | 在不同攝影機間切換 |

#### **UI / 渲染選項**

| 按鍵 | 功能 |
| :--- | :--- |
| **F1** | 顯示 / 隱藏幫助選單 (左側 UI) |
| **F2** | 顯示 / 隱藏資訊面板 (右側 UI) |
| **F3** | 顯示 / 隱藏渲染選項 |
| **F4** | 顯示 / 隱藏群組選項 |
| **F5** | 顯示 / 隱藏圖表 |
| **C** | 顯示 / 隱藏接觸點 |
| **T** | 顯示 / 隱藏透明物體 |
| **W** | 切換線框模式 |
| **S** | 顯示 / 隱藏陰影 |
| **L** | 顯示 / 隱藏燈光 |
| **R** | 顯示 / 隱藏參考座標系 |



#### 詳細說明在 xmls資料夾中所有檔案的用途及內容
好的，這沒有問題。`xmls` 資料夾是您整個 MuJoCo 模擬專案的**核心**，它定義了機器人的外觀、物理屬性以及它所在的虛擬世界。

以下是它們各自的用途及內容詳解。

---

### **第一部分：核心模型檔案**

這是定義機器人「本體」的檔案，包含了它的身體結構、關節、質量等。

#### 1. `open_duck_mini_v2.xml`
* **用途**：這是最核心的**機器人模型檔案**。它像一張詳細的藍圖，描述了機器人的每一個細節。
* **內容詳解**：
    * **`<compiler>`**: 設定編譯器選項，例如角度單位（`radian`）和網格檔案的路徑（`assets`）。
    * **`<default>`**: 定義了通用的預設值，例如所有關節的摩擦力、馬達的控制參數等，以避免重複撰寫。
    * **`<sensor>`**: 定義了所有感測器，如陀螺儀 (`gyro`)、加速度計 (`accelerometer`) 等，它們被附加在模型上以提供觀測數據。
    * **`<worldbody>`**: 這是檔案的核心。它使用巢狀的 `<body>` 標籤，像一棵樹一樣，從 `base` (基礎) 開始，逐級定義了軀幹 (`trunk_assembly`)、髖關節、大腿、膝關節、小腿、腳踝、腳掌以及頭部的所有連桿。
    * **`<inertial>`**: 在每一個 `<body>` 標籤內，這個標籤定義了該身體部件的物理屬性，包括質量 (`mass`)、重心位置 (`pos`) 和轉動慣量 (`fullinertia`)。
    * **`<joint>`**: 定義了關節的類型（例如 `hinge` 轉軸）、活動範圍 (`range`) 和名稱（例如 `left_hip_pitch`）。
    * **`<geom>`**: 定義了幾何形狀，用於視覺化 (`visual`) 和物理碰撞 (`collision`)。它會引用 `assets` 資料夾中的 3D 模型檔案（`.stl`）。
    * **`<actuator>`**: 定義了致動器（馬達）。它將一個馬達與一個特定的關節連結起來，並設定其控制參數（例如 `kp` 增益）。

#### 2. `open_duck_mini_v2_backlash.xml`
* **用途**：這是核心模型檔案的一個**特殊版本**，專門用來模擬**背隙 (Backlash)**。
* **內容詳解**：
    * 它與 `open_duck_mini_v2.xml` 的結構幾乎完全相同，唯一的關鍵區別在於：在每一個由馬達驅動的關節（如 `left_hip_yaw`）後面，都**額外**增加了一個同名的、帶有 `_backlash` 後綴的虛擬關節。
    * 這些背隙關節的活動範圍極小（約 ±0.5 度），用來模仿真實世界中齒輪之間的微小間隙，讓模擬更貼近物理現實。

---

### **第二部分：場景檔案 (Scene Files)**

這些檔案是「世界」或「環境」的定義。它們會**載入**一個核心模型檔案，然後將機器人放置到一個特定的場景中。

#### 3. `scene_flat_terrain.xml` & `scene_flat_terrain_backlash.xml`
* **用途**：定義一個**平坦地面**的訓練或測試場景。
* **內容詳解**：
    * **`<include>`**: 這是最重要的標籤。`scene_flat_terrain.xml` 會載入 `open_duck_mini_v2.xml`，而 `..._backlash.xml` 版本則會載入 `open_duck_mini_v2_backlash.xml`。
    * **`<visual>`**: 設定全域的視覺效果，如燈光、霧氣、預設視角等。
    * **`<asset>`**: 定義場景中使用的資源，例如地面網格的紋理貼圖。
    * **`<worldbody>`**: 在這裡，它定義了一個名為 `floor` 的物體，它是一個無限大的平面 (`type="plane"`)，作為機器人活動的地面。
    * **`<keyframe>`**: 定義了一個名為 `"home"` 的關鍵影格，其中包含了機器人初始姿態 (`qpos`) 和初始馬達控制指令 (`ctrl`) 的具體數值。

#### 4. `scene_rough_terrain_backlash.xml`
* **用途**：定義一個**崎嶇不平**的地面場景，用來訓練機器人應對複雜地形的能力。
* **內容詳解**：
    * 它同樣載入了 `open_duck_mini_v2_backlash.xml`。
    * 最大的不同在於，它使用了一個**高度場 (heightfield)** (`type="hfield"`) 來建立地面。這個高度場會引用 `assets/hfield.png` 這張灰階圖片，圖片的明暗會被轉換成地面的高低，從而形成崎嶇的地形。

#### 5. `scene_combined_terrain_backlash.xml`
* **用途**：定義一個**混合地形**場景，同時包含平地和崎嶇地面。
* **內容詳解**：
    * 它在 `<worldbody>` 中定義了**兩個**地面幾何體：一個是位於原點的平地 (`flat_floor`)，另一個是位於 x=20 位置的崎嶇高度場 (`rough_floor`)。

---

### **第三部分：模組化設定檔 (已整合)**

這兩個檔案的內容實際上已經被直接寫入了核心模型檔案中，但從概念上來說，它們是可被模組化的設定。

#### 6. `joints_properties.xml`
* **用途**：專門用來定義**關節和馬達的通用屬性**。
* **內容詳解**：
    * 它定義了名為 `sts3215` 的預設類別，其中包含了馬達的阻尼 (`damping`)、摩擦力 (`frictionloss`)、電樞 (`armature`) 和控制參數 (`position kp`) 等。
    * 同時也定義了名為 `backlash` 的預設類別，規定了所有背隙關節的物理屬性。

#### 7. `sensors.xml`
* **用途**：專門用來**定義所有的感測器**。
* **內容詳解**：
    * 包含了陀螺儀 (`gyro`)、加速度計 (`accelerometer`)、速度計 (`velocimeter`) 以及各種用於獲取身體或腳掌位置、姿態和速度的感測器。

希望這份詳細的解說能幫助您更好地理解這些 XML 檔案的結構和功能！


#### 如何解讀由 `plot_saved_obs.py` 產生的圖表。

---

### **圖表總覽**

這張圖表是您在執行 `mujoco_infer.py` 時，對機器人進行操作的**完整數據記錄**。它展示了在推論（inference）過程中，您下達的指令、機器人的實際反應以及其內部各個關節的狀態變化。

* **橫軸 (X軸) - Step**: 代表時間步，也就是您操作機器人的過程。
* **縱軸 (Y軸) - Value**: 代表特定感測器或指令的數值。

這張圖表主要分為三個部分，讓我們逐一分析：

### **第一部分：速度追蹤表現 (前三張圖)**

這三張圖是評估模型**任務完成度**的核心指標。它比較了您透過鍵盤下達的「目標指令」和機器人實際執行的「身體速度」之間的差距。

1.  **`lin_vel_x` (前後移動速度)**:
    * **橘線 (command)**: 代表您下達的「前進/後退」指令速度。
    * **藍線 (base)**: 代表機器人身體實際的「前進/後退」速度。
    * **解讀**: 在大部分時間裡，藍線都能很好地跟隨橘線的變化。這表示當您命令機器人前進或後退時，它能夠**非常精準地執行您的指令**。這是一個非常成功的訓練結果。

2.  **`lin_vel_y` (左右平移速度)**:
    * **橘線 (command)**: 代表您下達的「向左/向右」平移指令速度。
    * **藍線 (base)**: 代表機器人身體實際的「向左/向右」平移速度。
    * **解讀**: 同樣地，藍線緊緊跟隨著橘線，表明機器人對左右平移指令的響應也非常迅速和準確。

3.  **`ang_vel_z` (旋轉速度)**:
    * **橘線 (command)**: 代表您下達的「向左/向右」旋轉指令角速度。
    * **藍線 (base)**: 代表機器人身體實際的「向左/向右」旋轉角速度。
    * **解讀**: 旋轉控制同樣表現出色，模型能夠很好地理解並執行旋轉任務。

### **第二部分：關節角度 (`qpos`)**

這部分圖表展示了機器人各個主要關節在運動過程中的角度變化。您可以從中觀察到機器人走路的「姿態」。

* **例如 `left_hip_pitch` 和 `right_hip_pitch`**: 這兩條線代表左右髖關節的前後擺動。您可以看到它們呈現出**週期性**且**相位相反**的波動，這正是雙足行走時雙腿交替擺動的典型特徵。
* **例如 `left_knee` 和 `right_knee`**: 這兩條線代表膝關節的彎曲。它們的運動模式與髖關節相配合，共同構成了完整的行走步態。

**解讀**: 這些關節角度曲線的平滑度和協調性，直接反映了機器人步態的自然程度。從圖中平滑的週期性曲線來看，您的模型學會了一種穩定且協調的走路姿態。

### **第三部分：關節角速度 (`qvel`)**

這部分圖表顯示了各個關節轉動的「速度」。它通常用來判斷機器人的動作是否平順，有沒有劇烈的抖動。

* **解讀**: 您可以看到這些角速度曲線雖然有波動，但整體上是平滑的，沒有出現劇烈的、不規則的尖峰。這意味著機器人的動作是**流暢的**，而不是靠劇烈的、高頻的抖動來維持平衡。這再次證明了訓練的成功。


#### 如何設定最大的magnitude_range訓練範圍

好的，這是一個非常進階且關鍵的問題。要推薦 `magnitude_range` 的極限，我們需要像機器人工程師一樣，從硬體規格反推軟體參數的邊界。

我將為您分析這個機器人的**伺服機規格**、**重量**與**結構**，並基於這些物理限制，給您一個合理的`magnitude_range`極限訓練建議。

### **1. 伺服機規格分析**

在 `playground/open_duck_mini_v2/xmls/joints_properties.xml` 檔案中，定義了機器人所使用的馬達（致動器）預設屬性。其中，有一個關鍵的類別名稱：`class="sts3215"`。

這代表機器人的所有關節都使用了 **Feetech STS3215** 這款伺服機。透過查詢其官方規格，我們可以得知其核心性能：

* **堵轉扭矩 (Stall Torque)**: **15 kg·cm** (在 7.4V 電壓下)
    * **這是什麼意思？**：這是伺服機在被卡住無法轉動前，能輸出的**最大力矩**。將其換算成標準單位約為 **1.47 N·m (牛頓·米)**。這是我們計算極限最重要的依據。在持續運作中，我們通常只會使用其最大扭矩的 30% ~ 50% 以確保安全和壽命。

### **2. 機器人重量與結構分析**

我分析了 `playground/open_duck_mini_v2/xmls/open_duck_mini_v2.xml` 檔案中定義的所有身體部件的質量。

* **各部位質量總和**：
    * 軀幹 (trunk\_assembly): 0.408 kg
    * 髖關節 (hip) x2: ~0.12 kg
    * 大腿 (thigh) x2: ~0.11 kg
    * 小腿 (shin) x2: ~0.14 kg
    * 腳掌 (foot) x2: ~0.06 kg
    * 頭部 (head assembly): ~0.04 kg
* **機器人總重量**: 約 **0.88 kg**
* **軀幹質量 (TORSO\_MASS)**: **0.408 kg** (這是計算推力時的直接相關重量)
* **結構高度**: 機器人的初始站立高度 (`z` 軸) 約為 0.28 米。這可以作為推力施加點的力臂估算。

### **3. `magnitude_range` 極限範圍建議**

現在，我們將硬體規格和軟體參數聯繫起來。當一個大小為 `magnitude` 的加速度施加在軀幹上時，產生的力 `F` 為：

`F = TORSO_MASS * magnitude = 0.408 * magnitude`

這個力會對支撐腳的關節（主要是腳踝和髖關節）產生一個力矩 `T`，大小約為：

`T = F * 高度 = (0.408 * magnitude) * 0.28`

伺服機必須輸出至少同樣大小的力矩 `T` 才能抵銷這個推力而不摔倒。

#### **計算極限**

* **伺服機安全扭矩 (Stall Torque 的 50%)**: `1.47 N·m * 0.5 = 0.735 N·m`
* **伺服機極限扭矩 (Stall Torque 的 80%)**: `1.47 N·m * 0.8 = 1.176 N·m`

我們可以反推出對應的 `magnitude`：

* **安全 magnitude**: `0.735 / (0.408 * 0.28) ≈ 6.4 m/s²`
* **極限 magnitude**: `1.176 / (0.408 * 0.28) ≈ 10.3 m/s²`

#### **結論與建議**

考慮到機器人需要動態調整，而不僅僅是靜態對抗，以及多個關節協同工作，我為您提出以下 `magnitude_range` 訓練範圍建議：

1.  **積極訓練範圍 (Aggressive Range): `[3.0, 6.0]`**
    * 這個範圍的上限 (`6.0 m/s²`) 已經是一個非常大的推力（超過了重力加速度 g=9.8m/s² 的一半），足以迫使模型學會非常強健的平衡策略。
    * 在此範圍內，伺服機的負載仍在相對安全的區間，訓練出的模型泛化能力會很強。

2.  **極限挑戰範圍 (Hardcore Range): `[5.0, 10.0]`**
    * 這是一個非常具挑戰性的範圍，上限接近伺服機的物理極限。
    * **風險**: 訓練可能會變得不穩定且難以收斂。即使成功，訓練出的策略在真實機器人上執行時，可能會對伺服機造成較大損耗。
    * **收益**: 如果訓練成功，您將得到一個平衡能力極其出色的模型，能夠抵禦非常劇烈的外部干擾。

**總結建議**：
建議您從**積極訓練範圍 `[3.0, 6.0]`** 開始嘗試。先用這個範圍訓練出一個穩定的模型，然後再根據其表現，考慮是否要進一步挑戰 `[5.0, 10.0]` 這個更困難的極限範圍。